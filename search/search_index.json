{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"carefree-learn \u00b6 carefree-learn is a minimal Automatic Machine Learning (AutoML) solution for tabular datasets based on PyTorch . Why carefree-learn? \u00b6 carefree-learn Provides a scikit-learn -like interface with much more 'carefree' usages, including: Automatically deals with data pre-processing. Automatically handles datasets saved in files (.txt, .csv). Supports Distributed Training , which means hyper-parameter tuning can be very efficient in carefree-learn . Includes some brand new techniques which may boost vanilla Neural Network (NN) performances on tabular datasets, including: TreeDNN with Dynamic Soft Pruning , which makes NN less sensitive to hyper-parameters. Deep Distribution Regression (DDR) , which is capable of modeling the entire conditional distribution with one single NN model. Supports many convenient functionality in deep learning, including: Early stopping. Model persistence. Learning rate schedulers. And more... Full utilization of the WIP ecosystem cf* , such as: carefree-toolkit : provides a lot of utility classes & functions which are 'stand alone' and can be leveraged in your own projects. carefree-data : a lightweight tool to read -> convert -> process ANY tabular datasets. It also utilizes cython to accelerate critical procedures. From the above, it comes out that carefree-learn could be treated as a minimal Auto matic M achine L earning (AutoML) solution for tabular datasets when it is fully utilized. However, this is not built on the sacrifice of flexibility. In fact, the functionality we've mentioned are all wrapped into individual modules in carefree-learn and allow users to customize them easily. Installation \u00b6 carefree-learn requires Python 3.6 or higher. Pre-Installing PyTorch \u00b6 Please refer to PyTorch , and it is highly recommended to pre-install PyTorch with conda. pip installation \u00b6 After installing PyTorch, installation of carefree-learn would be rather easy: Tips: if you pre-installed PyTorch with conda, remember to activate the corresponding environment! pip install carefree-learn or git clone https://github.com/carefree0910/carefree-learn.git cd carefree-learn pip install -e . AutoML \u00b6 carefree-learn provides cflearn.Auto API for out-of-the-box usages. TL; DR \u00b6 import cflearn from cfdata.tabular import * # prepare iris dataset iris = TabularDataset . iris () iris = TabularData . from_dataset ( iris ) # split 10% of the data as validation data split = iris . split ( 0.1 ) train , valid = split . remained , split . split x_tr , y_tr = train . processed . xy x_cv , y_cv = valid . processed . xy data = x_tr , y_tr , x_cv , y_cv if __name__ == '__main__' : # standard usage fcnn = cflearn . make () . fit ( * data ) # 'overfit' validation set auto = cflearn . Auto ( TaskTypes . CLASSIFICATION ) . fit ( * data , num_jobs = 2 ) # estimate manually predictions = auto . predict ( x_cv ) print ( \"accuracy:\" , ( y_cv == predictions ) . mean ()) # estimate with `cflearn` cflearn . estimate ( x_cv , y_cv , pipelines = fcnn , other_patterns = { \"auto\" : auto . pattern }, ) Then you will see something like this: ================================================================================================================================ | metrics | acc | auc | -------------------------------------------------------------------------------------------------------------------------------- | | mean | std | score | mean | std | score | -------------------------------------------------------------------------------------------------------------------------------- | auto | -- 1.000000 -- | -- 0.000000 -- | -- 1.000000 -- | -- 1.000000 -- | -- 0.000000 -- | -- 1.000000 -- | -------------------------------------------------------------------------------------------------------------------------------- | fcnn | 0.933333 | -- 0.000000 -- | 0.933333 | 0.993333 | -- 0.000000 -- | 0.993333 | ================================================================================================================================ Explained \u00b6 cflearn.Auto.fit will run through the following steps: 1. fetch pre-defined hyper-parameters search space from OptunaPresetParams . 2. leverage optuna with cflearn.optuna_tune to perform hyper-parameters optimization. 3. use searched hyper-parameters to train multiple neural networks and ensemble them (with cflearn.ensemble or cflearn.Ensemble ). 4. record all these results to corresponding attributes. So after cflearn.Auto.fit , we can perform visualizations provided by optuna easily: export_folder = \"iris_vis\" auto . plot_param_importances ( export_folder = export_folder ) auto . plot_intermediate_values ( export_folder = export_folder ) Examples \u00b6 Here are some real life examples. Quick Start \u00b6 import cflearn from cfdata.tabular import TabularDataset x , y = TabularDataset . iris () . xy m = cflearn . make () . fit ( x , y ) # Make label predictions m . predict ( x ) # Make probability predictions m . predict_prob ( x ) # Estimate performance cflearn . estimate ( x , y , pipelines = m ) Then you will see something like this: ================================================================================================================================ | metrics | acc | auc | -------------------------------------------------------------------------------------------------------------------------------- | | mean | std | score | mean | std | score | -------------------------------------------------------------------------------------------------------------------------------- | fcnn | 0.946667 | 0.000000 | 0.946667 | 0.993200 | 0.000000 | 0.993200 | ================================================================================================================================ It is also worth mentioning that carefree-learn models can be saved easily, into a zip file! For example, a cflearn^_^fcnn.zip file will be created with one line of code: cflearn . save ( m ) Of course, loading carefree-learn models are easy too! m = cflearn . load () # You will see exactly the same result as above! cflearn . estimate ( x , y , pipelines = m ) carefree-learn can also easily fit / predict / estimate directly on files (file-in, file-out). Suppose we have an 'xor.txt' file with following contents: 0,0,0 0,1,1 1,0,1 1,1,0 Then carefree-learn can be utilized with only one line of code: delim refers to 'delimiter', and skip_first refers to whether skipping the first line or not. Please refer to carefree-data if you're interested in more details. m = cflearn . make ( delim = \",\" , skip_first = False ) . fit ( \"xor.txt\" , x_cv = \"xor.txt\" ) cflearn . estimate ( \"xor.txt\" , pipelines = m ) After which you will see something like this: ================================================================================================================================ | metrics | acc | auc | -------------------------------------------------------------------------------------------------------------------------------- | | mean | std | score | mean | std | score | -------------------------------------------------------------------------------------------------------------------------------- | fcnn | 1.000000 | 0.000000 | 1.000000 | 1.000000 | 0.000000 | 1.000000 | ================================================================================================================================ When we fit from files, we can predict on either files or lists: print ( m . predict ([[ 0 , 0 ]])) # [[0]] print ( m . predict ([[ 0 , 1 ]])) # [[1]] print ( m . predict ( \"xor.txt\" )) # [ [0] [1] [1] [0] ] Info The maximum number of checkpoint files can be specified. See max_snapshot_num in cflearn.make for more details. Distributed \u00b6 In carefree-learn , Distributed Training doesn't mean training your model on multiple GPUs or multiple machines, because carefree-learn focuses on tabular datasets (or, structured datasets) which are often not as large as unstructured datasets. Instead, Distributed Training in carefree-learn means training multiple models at the same time. This is important because: Deep Learning models suffer from randomness, so we need to train multiple models with the same algorithm and calculate the mean / std of the performances to estimate the algorithm's capacity and stability. Ensemble these models (which are trained with the same algorithm) can boost the algorithm's performance without making any changes to the algorithm itself. Parameter searching will be easier & faster. import cflearn from cfdata.tabular import TabularDataset # It is necessary to wrap codes under '__main__' on WINDOWS platform when running distributed codes if __name__ == '__main__' : x , y = TabularDataset . iris () . xy # Notice that 3 fcnn were trained simultaneously with this line of code results = cflearn . repeat_with ( x , y , num_repeat = 3 , num_jobs = 0 ) patterns = results . patterns [ \"fcnn\" ] # And it is fairly straight forward to apply stacking ensemble ensemble = cflearn . ensemble ( patterns ) patterns_dict = { \"fcnn_3\" : patterns , \"fcnn_3_ensemble\" : ensemble } cflearn . estimate ( x , y , metrics = [ \"acc\" , \"auc\" ], other_patterns = patterns_dict ) Then you will see something like this: ================================================================================================================================ | metrics | acc | auc | -------------------------------------------------------------------------------------------------------------------------------- | | mean | std | score | mean | std | score | -------------------------------------------------------------------------------------------------------------------------------- | fcnn_3 | 0.937778 | 0.017498 | 0.920280 | -- 0.993911 -- | 0.000274 | 0.993637 | -------------------------------------------------------------------------------------------------------------------------------- | fcnn_3_ensemble | -- 0.953333 -- | -- 0.000000 -- | -- 0.953333 -- | 0.993867 | -- 0.000000 -- | -- 0.993867 -- | ================================================================================================================================ Info You might notice that the best results of each column is 'highlighted' with a pair of '--'. Hyper Parameter Optimization (HPO) \u00b6 import cflearn from cfdata.tabular import * if __name__ == '__main__' : x , y = TabularDataset . iris () . xy # Bayesian Optimization (BO) will be used as default hpo = cflearn . tune_with ( x , y , task_type = TaskTypes . CLASSIFICATION , num_repeat = 2 , num_parallel = 0 , num_search = 10 ) # We can further train our model with the best hyper-parameters we've obtained: m = cflearn . make ( ** hpo . best_param ) . fit ( x , y ) cflearn . estimate ( x , y , pipelines = m ) Then you will see something like this: ~~~ [ info ] Results ================================================================================================================================ | metrics | acc | auc | -------------------------------------------------------------------------------------------------------------------------------- | | mean | std | score | mean | std | score | -------------------------------------------------------------------------------------------------------------------------------- | 0659e09f | 0.943333 | 0.016667 | 0.926667 | 0.995500 | 0.001967 | 0.993533 | -------------------------------------------------------------------------------------------------------------------------------- | 08a0a030 | 0.796667 | 0.130000 | 0.666667 | 0.969333 | 0.012000 | 0.957333 | -------------------------------------------------------------------------------------------------------------------------------- | 1962285c | 0.950000 | 0.003333 | 0.946667 | 0.997467 | 0.000533 | 0.996933 | -------------------------------------------------------------------------------------------------------------------------------- | 1eb7f2a0 | 0.933333 | 0.020000 | 0.913333 | 0.994833 | 0.003033 | 0.991800 | -------------------------------------------------------------------------------------------------------------------------------- | 4ed5bb3b | 0.973333 | 0.013333 | 0.960000 | 0.998733 | 0.000467 | 0.998267 | -------------------------------------------------------------------------------------------------------------------------------- | 5a652f3c | 0.953333 | -- 0.000000 -- | 0.953333 | 0.997400 | 0.000133 | 0.997267 | -------------------------------------------------------------------------------------------------------------------------------- | 82c35e77 | 0.940000 | 0.020000 | 0.920000 | 0.995467 | 0.002133 | 0.993333 | -------------------------------------------------------------------------------------------------------------------------------- | a9ef52d0 | -- 0.986667 -- | 0.006667 | -- 0.980000 -- | -- 0.999200 -- | -- 0.000000 -- | -- 0.999200 -- | -------------------------------------------------------------------------------------------------------------------------------- | ba2e179a | 0.946667 | 0.026667 | 0.920000 | 0.995633 | 0.001900 | 0.993733 | -------------------------------------------------------------------------------------------------------------------------------- | ec8c0837 | 0.973333 | -- 0.000000 -- | 0.973333 | 0.998867 | 0.000067 | 0.998800 | ================================================================================================================================ ~~~ [ info ] Best Parameters ---------------------------------------------------------------------------------------------------- acc (a9ef52d0) (0.986667 \u00b1 0.006667) ---------------------------------------------------------------------------------------------------- {'optimizer': 'rmsprop', 'optimizer_config': {'lr': 0.005810863965757382}} ---------------------------------------------------------------------------------------------------- auc (a9ef52d0) (0.999200 \u00b1 0.000000) ---------------------------------------------------------------------------------------------------- {'optimizer': 'rmsprop', 'optimizer_config': {'lr': 0.005810863965757382}} ---------------------------------------------------------------------------------------------------- best (a9ef52d0) ---------------------------------------------------------------------------------------------------- {'optimizer': 'rmsprop', 'optimizer_config': {'lr': 0.005810863965757382}} ---------------------------------------------------------------------------------------------------- ~~ [ info ] Results ================================================================================================================================ | metrics | acc | auc | -------------------------------------------------------------------------------------------------------------------------------- | | mean | std | score | mean | std | score | -------------------------------------------------------------------------------------------------------------------------------- | fcnn | 0.980000 | 0.000000 | 0.980000 | 0.998867 | 0.000000 | 0.998867 | ================================================================================================================================ You might notice that: The final results obtained by HPO is even better than the stacking ensemble results mentioned above. We search for optimizer and lr as default. In fact, we can manually passed params into cflearn.tune_with . If not, then carefree-learn will execute following codes: from cftool.ml.param_utils import * params = { \"optimizer\" : String ( Choice ( values = [ \"sgd\" , \"rmsprop\" , \"adam\" ])), \"optimizer_config\" : { \"lr\" : Float ( Exponential ( 1e-5 , 0.1 )) } } Info It is also worth mentioning that we can pass file datasets into cflearn.tune_with as well. See tests/usages/test_basic.py for more details. Citation \u00b6 If you use carefree-learn in your research, we would greatly appreciate if you cite this library using this Bibtex: @misc{carefree-learn, year={2020}, author={Yujian He}, title={carefree-learn, a minimal Automatic Machine Learning (AutoML) solution for tabular datasets based on PyTorch}, howpublished={\\url{https://https://github.com/carefree0910/carefree-learn/}}, } License \u00b6 carefree-learn is MIT licensed, as found in the LICENSE file.","title":"Home"},{"location":"#carefree-learn","text":"carefree-learn is a minimal Automatic Machine Learning (AutoML) solution for tabular datasets based on PyTorch .","title":"carefree-learn"},{"location":"#why-carefree-learn","text":"carefree-learn Provides a scikit-learn -like interface with much more 'carefree' usages, including: Automatically deals with data pre-processing. Automatically handles datasets saved in files (.txt, .csv). Supports Distributed Training , which means hyper-parameter tuning can be very efficient in carefree-learn . Includes some brand new techniques which may boost vanilla Neural Network (NN) performances on tabular datasets, including: TreeDNN with Dynamic Soft Pruning , which makes NN less sensitive to hyper-parameters. Deep Distribution Regression (DDR) , which is capable of modeling the entire conditional distribution with one single NN model. Supports many convenient functionality in deep learning, including: Early stopping. Model persistence. Learning rate schedulers. And more... Full utilization of the WIP ecosystem cf* , such as: carefree-toolkit : provides a lot of utility classes & functions which are 'stand alone' and can be leveraged in your own projects. carefree-data : a lightweight tool to read -> convert -> process ANY tabular datasets. It also utilizes cython to accelerate critical procedures. From the above, it comes out that carefree-learn could be treated as a minimal Auto matic M achine L earning (AutoML) solution for tabular datasets when it is fully utilized. However, this is not built on the sacrifice of flexibility. In fact, the functionality we've mentioned are all wrapped into individual modules in carefree-learn and allow users to customize them easily.","title":"Why carefree-learn?"},{"location":"#installation","text":"carefree-learn requires Python 3.6 or higher.","title":"Installation"},{"location":"#pre-installing-pytorch","text":"Please refer to PyTorch , and it is highly recommended to pre-install PyTorch with conda.","title":"Pre-Installing PyTorch"},{"location":"#pip-installation","text":"After installing PyTorch, installation of carefree-learn would be rather easy: Tips: if you pre-installed PyTorch with conda, remember to activate the corresponding environment! pip install carefree-learn or git clone https://github.com/carefree0910/carefree-learn.git cd carefree-learn pip install -e .","title":"pip installation"},{"location":"#automl","text":"carefree-learn provides cflearn.Auto API for out-of-the-box usages.","title":"AutoML"},{"location":"#tl-dr","text":"import cflearn from cfdata.tabular import * # prepare iris dataset iris = TabularDataset . iris () iris = TabularData . from_dataset ( iris ) # split 10% of the data as validation data split = iris . split ( 0.1 ) train , valid = split . remained , split . split x_tr , y_tr = train . processed . xy x_cv , y_cv = valid . processed . xy data = x_tr , y_tr , x_cv , y_cv if __name__ == '__main__' : # standard usage fcnn = cflearn . make () . fit ( * data ) # 'overfit' validation set auto = cflearn . Auto ( TaskTypes . CLASSIFICATION ) . fit ( * data , num_jobs = 2 ) # estimate manually predictions = auto . predict ( x_cv ) print ( \"accuracy:\" , ( y_cv == predictions ) . mean ()) # estimate with `cflearn` cflearn . estimate ( x_cv , y_cv , pipelines = fcnn , other_patterns = { \"auto\" : auto . pattern }, ) Then you will see something like this: ================================================================================================================================ | metrics | acc | auc | -------------------------------------------------------------------------------------------------------------------------------- | | mean | std | score | mean | std | score | -------------------------------------------------------------------------------------------------------------------------------- | auto | -- 1.000000 -- | -- 0.000000 -- | -- 1.000000 -- | -- 1.000000 -- | -- 0.000000 -- | -- 1.000000 -- | -------------------------------------------------------------------------------------------------------------------------------- | fcnn | 0.933333 | -- 0.000000 -- | 0.933333 | 0.993333 | -- 0.000000 -- | 0.993333 | ================================================================================================================================","title":"TL; DR"},{"location":"#explained","text":"cflearn.Auto.fit will run through the following steps: 1. fetch pre-defined hyper-parameters search space from OptunaPresetParams . 2. leverage optuna with cflearn.optuna_tune to perform hyper-parameters optimization. 3. use searched hyper-parameters to train multiple neural networks and ensemble them (with cflearn.ensemble or cflearn.Ensemble ). 4. record all these results to corresponding attributes. So after cflearn.Auto.fit , we can perform visualizations provided by optuna easily: export_folder = \"iris_vis\" auto . plot_param_importances ( export_folder = export_folder ) auto . plot_intermediate_values ( export_folder = export_folder )","title":"Explained"},{"location":"#examples","text":"Here are some real life examples.","title":"Examples"},{"location":"#quick-start","text":"import cflearn from cfdata.tabular import TabularDataset x , y = TabularDataset . iris () . xy m = cflearn . make () . fit ( x , y ) # Make label predictions m . predict ( x ) # Make probability predictions m . predict_prob ( x ) # Estimate performance cflearn . estimate ( x , y , pipelines = m ) Then you will see something like this: ================================================================================================================================ | metrics | acc | auc | -------------------------------------------------------------------------------------------------------------------------------- | | mean | std | score | mean | std | score | -------------------------------------------------------------------------------------------------------------------------------- | fcnn | 0.946667 | 0.000000 | 0.946667 | 0.993200 | 0.000000 | 0.993200 | ================================================================================================================================ It is also worth mentioning that carefree-learn models can be saved easily, into a zip file! For example, a cflearn^_^fcnn.zip file will be created with one line of code: cflearn . save ( m ) Of course, loading carefree-learn models are easy too! m = cflearn . load () # You will see exactly the same result as above! cflearn . estimate ( x , y , pipelines = m ) carefree-learn can also easily fit / predict / estimate directly on files (file-in, file-out). Suppose we have an 'xor.txt' file with following contents: 0,0,0 0,1,1 1,0,1 1,1,0 Then carefree-learn can be utilized with only one line of code: delim refers to 'delimiter', and skip_first refers to whether skipping the first line or not. Please refer to carefree-data if you're interested in more details. m = cflearn . make ( delim = \",\" , skip_first = False ) . fit ( \"xor.txt\" , x_cv = \"xor.txt\" ) cflearn . estimate ( \"xor.txt\" , pipelines = m ) After which you will see something like this: ================================================================================================================================ | metrics | acc | auc | -------------------------------------------------------------------------------------------------------------------------------- | | mean | std | score | mean | std | score | -------------------------------------------------------------------------------------------------------------------------------- | fcnn | 1.000000 | 0.000000 | 1.000000 | 1.000000 | 0.000000 | 1.000000 | ================================================================================================================================ When we fit from files, we can predict on either files or lists: print ( m . predict ([[ 0 , 0 ]])) # [[0]] print ( m . predict ([[ 0 , 1 ]])) # [[1]] print ( m . predict ( \"xor.txt\" )) # [ [0] [1] [1] [0] ] Info The maximum number of checkpoint files can be specified. See max_snapshot_num in cflearn.make for more details.","title":"Quick Start"},{"location":"#distributed","text":"In carefree-learn , Distributed Training doesn't mean training your model on multiple GPUs or multiple machines, because carefree-learn focuses on tabular datasets (or, structured datasets) which are often not as large as unstructured datasets. Instead, Distributed Training in carefree-learn means training multiple models at the same time. This is important because: Deep Learning models suffer from randomness, so we need to train multiple models with the same algorithm and calculate the mean / std of the performances to estimate the algorithm's capacity and stability. Ensemble these models (which are trained with the same algorithm) can boost the algorithm's performance without making any changes to the algorithm itself. Parameter searching will be easier & faster. import cflearn from cfdata.tabular import TabularDataset # It is necessary to wrap codes under '__main__' on WINDOWS platform when running distributed codes if __name__ == '__main__' : x , y = TabularDataset . iris () . xy # Notice that 3 fcnn were trained simultaneously with this line of code results = cflearn . repeat_with ( x , y , num_repeat = 3 , num_jobs = 0 ) patterns = results . patterns [ \"fcnn\" ] # And it is fairly straight forward to apply stacking ensemble ensemble = cflearn . ensemble ( patterns ) patterns_dict = { \"fcnn_3\" : patterns , \"fcnn_3_ensemble\" : ensemble } cflearn . estimate ( x , y , metrics = [ \"acc\" , \"auc\" ], other_patterns = patterns_dict ) Then you will see something like this: ================================================================================================================================ | metrics | acc | auc | -------------------------------------------------------------------------------------------------------------------------------- | | mean | std | score | mean | std | score | -------------------------------------------------------------------------------------------------------------------------------- | fcnn_3 | 0.937778 | 0.017498 | 0.920280 | -- 0.993911 -- | 0.000274 | 0.993637 | -------------------------------------------------------------------------------------------------------------------------------- | fcnn_3_ensemble | -- 0.953333 -- | -- 0.000000 -- | -- 0.953333 -- | 0.993867 | -- 0.000000 -- | -- 0.993867 -- | ================================================================================================================================ Info You might notice that the best results of each column is 'highlighted' with a pair of '--'.","title":"Distributed"},{"location":"#hyper-parameter-optimization-hpo","text":"import cflearn from cfdata.tabular import * if __name__ == '__main__' : x , y = TabularDataset . iris () . xy # Bayesian Optimization (BO) will be used as default hpo = cflearn . tune_with ( x , y , task_type = TaskTypes . CLASSIFICATION , num_repeat = 2 , num_parallel = 0 , num_search = 10 ) # We can further train our model with the best hyper-parameters we've obtained: m = cflearn . make ( ** hpo . best_param ) . fit ( x , y ) cflearn . estimate ( x , y , pipelines = m ) Then you will see something like this: ~~~ [ info ] Results ================================================================================================================================ | metrics | acc | auc | -------------------------------------------------------------------------------------------------------------------------------- | | mean | std | score | mean | std | score | -------------------------------------------------------------------------------------------------------------------------------- | 0659e09f | 0.943333 | 0.016667 | 0.926667 | 0.995500 | 0.001967 | 0.993533 | -------------------------------------------------------------------------------------------------------------------------------- | 08a0a030 | 0.796667 | 0.130000 | 0.666667 | 0.969333 | 0.012000 | 0.957333 | -------------------------------------------------------------------------------------------------------------------------------- | 1962285c | 0.950000 | 0.003333 | 0.946667 | 0.997467 | 0.000533 | 0.996933 | -------------------------------------------------------------------------------------------------------------------------------- | 1eb7f2a0 | 0.933333 | 0.020000 | 0.913333 | 0.994833 | 0.003033 | 0.991800 | -------------------------------------------------------------------------------------------------------------------------------- | 4ed5bb3b | 0.973333 | 0.013333 | 0.960000 | 0.998733 | 0.000467 | 0.998267 | -------------------------------------------------------------------------------------------------------------------------------- | 5a652f3c | 0.953333 | -- 0.000000 -- | 0.953333 | 0.997400 | 0.000133 | 0.997267 | -------------------------------------------------------------------------------------------------------------------------------- | 82c35e77 | 0.940000 | 0.020000 | 0.920000 | 0.995467 | 0.002133 | 0.993333 | -------------------------------------------------------------------------------------------------------------------------------- | a9ef52d0 | -- 0.986667 -- | 0.006667 | -- 0.980000 -- | -- 0.999200 -- | -- 0.000000 -- | -- 0.999200 -- | -------------------------------------------------------------------------------------------------------------------------------- | ba2e179a | 0.946667 | 0.026667 | 0.920000 | 0.995633 | 0.001900 | 0.993733 | -------------------------------------------------------------------------------------------------------------------------------- | ec8c0837 | 0.973333 | -- 0.000000 -- | 0.973333 | 0.998867 | 0.000067 | 0.998800 | ================================================================================================================================ ~~~ [ info ] Best Parameters ---------------------------------------------------------------------------------------------------- acc (a9ef52d0) (0.986667 \u00b1 0.006667) ---------------------------------------------------------------------------------------------------- {'optimizer': 'rmsprop', 'optimizer_config': {'lr': 0.005810863965757382}} ---------------------------------------------------------------------------------------------------- auc (a9ef52d0) (0.999200 \u00b1 0.000000) ---------------------------------------------------------------------------------------------------- {'optimizer': 'rmsprop', 'optimizer_config': {'lr': 0.005810863965757382}} ---------------------------------------------------------------------------------------------------- best (a9ef52d0) ---------------------------------------------------------------------------------------------------- {'optimizer': 'rmsprop', 'optimizer_config': {'lr': 0.005810863965757382}} ---------------------------------------------------------------------------------------------------- ~~ [ info ] Results ================================================================================================================================ | metrics | acc | auc | -------------------------------------------------------------------------------------------------------------------------------- | | mean | std | score | mean | std | score | -------------------------------------------------------------------------------------------------------------------------------- | fcnn | 0.980000 | 0.000000 | 0.980000 | 0.998867 | 0.000000 | 0.998867 | ================================================================================================================================ You might notice that: The final results obtained by HPO is even better than the stacking ensemble results mentioned above. We search for optimizer and lr as default. In fact, we can manually passed params into cflearn.tune_with . If not, then carefree-learn will execute following codes: from cftool.ml.param_utils import * params = { \"optimizer\" : String ( Choice ( values = [ \"sgd\" , \"rmsprop\" , \"adam\" ])), \"optimizer_config\" : { \"lr\" : Float ( Exponential ( 1e-5 , 0.1 )) } } Info It is also worth mentioning that we can pass file datasets into cflearn.tune_with as well. See tests/usages/test_basic.py for more details.","title":"Hyper Parameter Optimization (HPO)"},{"location":"#citation","text":"If you use carefree-learn in your research, we would greatly appreciate if you cite this library using this Bibtex: @misc{carefree-learn, year={2020}, author={Yujian He}, title={carefree-learn, a minimal Automatic Machine Learning (AutoML) solution for tabular datasets based on PyTorch}, howpublished={\\url{https://https://github.com/carefree0910/carefree-learn/}}, }","title":"Citation"},{"location":"#license","text":"carefree-learn is MIT licensed, as found in the LICENSE file.","title":"License"},{"location":"introduction/","text":"Introduction \u00b6 Advantages \u00b6 Like many similar projects, carefree-learn can be treated as a high-level library to help with training neural networks in PyTorch. However, carefree-learn does less and more than that. carefree-learn focuses on tabular (structured) datasets, instead of unstructured datasets (e.g. NLP datasets or CV datasets). carefree-learn provides an end-to-end pipeline on tabular datasets, including AUTOMATICALLY deal with (this part is mainly handled by carefree-data , though): Detection of redundant feature columns which can be excluded (all SAME, all DIFFERENT, etc). Detection of feature columns types (whether a feature column is string column / numerical column / categorical column). Imputation of missing values. Encoding of string columns and categorical columns (Embedding or One Hot Encoding). Pre-processing of numerical columns (Normalize, Min Max, etc.). And much more... carefree-learn can help you deal with almost ANY kind of tabular datasets, no matter how 'dirty' and 'messy' it is. It can be either trained directly with some numpy arrays, or trained indirectly with some files locate on your machine. This makes carefree-learn stand out from similar projects. carefree-learn is highly customizable for developers. We have already wrapped (almost) every single functionality / process into a single module (a Python class), and they can be replaced or enhanced either directly from source codes or from local codes with the help of some pre-defined functions provided by carefree-learn (see Registration ). carefree-learn supports easy-to-use saving and loading. By default, everything will be wrapped into a zip file! carefree-learn supports Distributed Training . Info From the discriptions above, you might notice that carefree-learn is more of a minimal Automatic Machine Learning (AutoML) solution than a pure Machine Learning package. Tip When we say ANY , it means that carefree-learn can even train on dataset with only one single sample: import cflearn toy = cflearn . make_toy_model () data = toy . tr_data . converted print ( f \"x= { data . x } , y= { data . y } \" ) # x=[[0.]], y=[[1.]] This is especially useful when we need to do unittests or to verify whether our custom modules (e.g. custom pre-processes) are correctly integrated into carefree-learn , for example: import cflearn import numpy as np # here we implement a custom processor @cflearn . register_processor ( \"plus_one\" ) class PlusOne ( cflearn . Processor ): @property def input_dim ( self ) -> int : return 1 @property def output_dim ( self ) -> int : return 1 def fit ( self , columns : np . ndarray ) -> cflearn . Processor : return self def _process ( self , columns : np . ndarray ) -> np . ndarray : return columns + 1 def _recover ( self , processed_columns : np . ndarray ) -> np . ndarray : return processed_columns - 1 # we need to specify that we use the custom process method to process our labels config = { \"data_config\" : { \"label_process_method\" : \"plus_one\" }} toy = cflearn . make_toy_model ( config ) y = toy . tr_data . converted . y processed_y = toy . tr_data . processed . y print ( f \"y= { y } , new_y= { processed_y } \" ) # y=[[1.]], new_y=[[2.]] There is one more thing we'd like to mention: carefree-learn is ' Pandas -free'. The reasons why we excluded Pandas are listed in carefree-data . Design Tenets \u00b6 carefree-learn was designed to support most commonly used methods with 'carefree' APIs. Moreover, carefree-learn was also designed with interface which is general enough, so that more sophisticated functionality can also be easily integrated in the future. This brings a tension in how to create abstractions in code, which is a challenge for us: On the one hand, it requires a reasonably high-level abstraction so that users can easily work around with it in a standard way, without having to worry too much about the details. On the other hand, it also needs to have a very thin abstraction to allow users to do (many) other things in new ways. Breaking existing abstractions and replacing them with new ones should be fairly easy. In carefree-learn , there are three main design tenets that address this tension together: Share configurations with one single config argument (see Configurations ). Build some common blocks which shall be leveraged across different models (see Common Blocks ). Divide carefree-learn into three parts: Model , Trainer and Pipeline , each focuses on certain roles. Implemente functions ( cflearn.register_* to be exact) to ensure flexibility and control on different modules and stuffs (see Registration ). We will introduce the details in the following subsections. Common Blocks \u00b6 Info Source codes path: blocks.py . Commonality is important for abstractions. When it comes to deep learning, it is not difficult to figure out the very common structure across all models: the Mapping Layers which is responsible for mapping data distrubution from one dimensional to another. Although some efforts have been made to replace the Mapping Layers (e.g. DNDF 1 ), it is undeniable that the Mapping Layers should be extracted as a stand-alone module before any other structures. But in carefree-learn , we should do more than that. Recall that carefree-learn focuses on tabular datasets, which means carefree-learn will use Mapping Layers in most cases (Unlike CNN or RNN which has Convolutional Blocks and RNNCell Blocks respectively). In this case, it is necessary to wrap multiple Mapping s into one single Module - also well known as MLP - in carefree-learn . So, in CNN we have Conv2D , in RNN we have LSTM , and in carefree-learn we have MLP . Model \u00b6 Info Source codes path: base.py -> class ModelBase . In carefree-learn , a Model should implement the core algorithms. It assumes that the input data in training process is already 'batched, processed, nice and clean', but not yet 'encoded'. Fortunately, carefree-learn pre-defined some useful methods which can encode categorical columns easily. It does not care about how to train a model, it only focuses on how to make predictions with input, and how to calculate losses with them. Note Model s are likely to define MLP blocks frequently, as explained in the Common Blocks section. Trainer \u00b6 Info Source codes path: core.py -> class Trainer . In carefree-learn , a Trainer should implement the high-level parts, as listed below: It assumes that the input data is already 'processed, nice and clean', but it should take care of getting input data into batches, because in real applications batching is essential for performance. It should take care of the training loop, which includes updating parameters with an optimizer, verbosing metrics, checkpointing, early stopping, logging, etc. Pipeline \u00b6 Info Source codes path: core.py -> class Pipeline . In carefree-learn , a Pipeline should implement the preparation and API part. It should not make any assumptions to the input data, it could already be 'nice and clean', but it could also be 'dirty and messy'. Therefore, it needs to transform the original data into 'nice and clean' data and then feed it to Trainer . The data transformations include (this part is mainly handled by carefree-data , though): Imputation of missing values. Transforming string columns into categorical columns. Processing numerical columns. Processing label column (if needed). It should implement some algorithm-agnostic functions (e.g. predict , save , load , etc.). Registration \u00b6 Registration in carefree-learn means registering user-defined modules to carefree-learn , so carefree-learn can leverage these modules to resolve more specific tasks. In most cases, the registration stuffs are done by simply defining and updating many global dict s. For example, carefree-learn defined some useful parameter initializations in cflearn.misc.toolkit.Initializer . If we want to use our own initialization methods, simply register it and then everything will be fine: import torch import cflearn from torch.nn import Parameter initializer = cflearn . Initializer ({}) @cflearn . register_initializer ( \"all_one\" ) def all_one ( initializer_ , parameter ): parameter . fill_ ( 1. ) param = Parameter ( torch . zeros ( 3 )) with torch . no_grad (): initializer . initialize ( param , \"all_one\" ) print ( param ) # tensor([1., 1., 1.], requires_grad=True) Info Currently we mainly have 5 registrations in use: register_metric , register_optimizer , register_scheduler , register_initializer and register_processor Configurations \u00b6 In carefree-learn , we have few args and kwargs in each module. Instead, we'll use one single argument config which takes in a (shared) Python dict to configure those modules. That's why we can easily support JSON configuration, which is very useful when you need to share your models to others or reproduce others' work. Scopes \u00b6 Since we have many stand-alone modules that provide corresponding functionalities, our configuration (which is a Python dict) will be designed 'hierarchically', and each module can read its specified configuration under its specific 'scope'. If needed, they can also access configurations defined under other 'scopes' easily because the whole configuration dict will be passed to each module. Info Currently we mainly have 6 scopes in use: root ( pipeline_config ), model_config , trainer_config , data_config , metric_config and optimizers . Suppose we have a Python dict named config now: The keys in root scope of config are those which are directly stored in the config . The keys in other scopes (e.g. data_config ) of config are those which are stored in a sub-dict, and this sub-dict is the value of the scope-name-key in config (e.g. \"data_config\" ). Here is an example: config = { # `root` scope \"foo\" : ... , \"dummy\" : ... , # `model_config` scope \"model_config\" : { \"...\" : ... }, # `trainer_config` scope \"trainer_config\" : { \"...\" : ... }, ... } We will introduce what kinds of algorithm-agnostic configurations are available in each scope and how to specify them in the Configurations section. Data Loading Strategy \u00b6 Since carefree-learn focuses on tabular datasets, the data loading strategy is very different from unstructured datasets' strategy. For instance, it is quite common that a CV dataset is a bunch of pictures located in a folder, and we will either read them sequentially or read them in parallel. Nowadays, almost every famous deep learning framework has their own solution to load unstructured datasets efficiently, e.g. PyTorch officially implements DataLoader to support multi-process loading and other features. Although we know that RAM speed is (almost) always faster than I/O operations, we still prefer leveraging multi-process to read files than loading them all into RAM at once. This is because unstructured datasets are often too large to allocate them all to RAM. However, when it comes to tabular datasets, we prefer to load everything into RAM at the very beginning. The main reasons are listed below: Tabular datasets are often quite small and are able to put into RAM at once. Network structures for tabular datasets are often much smaller, which means using multi-process loading will cause a much heavier overhead. We need to take Distributed Training into account. If we stick to multi-process loading, there would be too many threads in the pool which is not a good practice. Distributed Training \u00b6 In carefree-learn , Distributed Training doesn't mean training your model on multiple GPUs or multiple machines, because carefree-learn focuses on tabular datasets which are often not as large as unstructured datasets. Instead, Distributed Training in carefree-learn means training multiple models at the same time. This is important because: Deep Learning models suffer from randomness, so we need to train multiple models with the same algorithm and calculate the mean / std of the performances to estimate the algorithm's capacity and stability. Ensembling these models (trained with the same algorithm) can boost the algorithm's performance without making any changes to the algorithm itself. Parameter searching will be easier & faster. Terminologies \u00b6 In carefree-learn , there are some frequently used terminologies, and we will introduce them in this section. If you are confused by some other terminologies in carefree-learn when you are using it, feel free to edit this list: step \u00b6 One step in the training process means that one mini-batch passed through our model. epochs \u00b6 In most deep learning processes, training is structured into epochs. An epoch is one iteration over the entire input data, which is constructed by several step s. batch_size \u00b6 It is a good practice to slice the data into smaller batches and iterates over these batches during training, and batch_size specifies the size of each batch. Be aware that the last batch may be smaller if the total number of samples is not divisible by the batch_size . config \u00b6 A config indicates the main part (or, the shared part) of the configuration. increment_config \u00b6 An increment_config indicates the configurations that you want to update on config . Info This is very useful when you only want to tune a single configuration and yet you have tons of other configurations need to be fixed. In this case, you can set other configurations as config , and adjust the target configuration in increment_config . forward \u00b6 A forward method is a common method required by (almost) all PyTorch modules. Info Here is a nice discussion. task_type \u00b6 We use task_type = \"clf\" to indicate a classification task, and task_type = \"reg\" to indicate a regression task. Info And we'll convert them into cfdata . tabular . TaskTypes under the hood. tr, cv & te \u00b6 In most cases, we use: x_tr , x_cv and x_te to represent training , cross validation and test features . y_tr , y_cv and y_te to represent training , cross validation and test labels . metrics \u00b6 Although losses are what we optimize directly during training, metrics are what we 'actually' want to optimize (e.g. acc , auc , f1-score , etc.). Sometimes we may want to take multiple metrics into consideration, and we may also want to eliminate the fluctuation comes with mini-batch training by applying EMA on the metrics. To make things clearer, we decided to introduce the metric_config scope (under the trainer_config scope). By default: mae & mse is used for regression tasks, while auc & acc is used for classification tasks. An EMA with decay = 0.1 will be used. Every metrics will be treated as equal. So carefree-learn will construct the following configurations for you by default (take classification tasks as an example): { ..., \"trainer_config\" : { ..., \"metric_config\" : { \"decay\" : 0.1 , \"types\" : [ \"auc\" , \"acc\" ], \"weights\" : { \"auc\" : 1.0 , \"acc\" : 1.0 } } } } It's worth mentioning that carefree-learn also supports using losses as metrics: { ..., \"trainer_config\" : { ..., \"metric_config\" : { \"decay\" : 0.1 , \"types\" : [ \"loss\" ] } } } optimizers \u00b6 Sometimes we may want to have different optimizers to optimize different group of parameters. In order to make things easier with flexibility and control, we decided to introduce the optimizers scope (under the trainer_config scope). By default, all parameters will be optimized via one single optimizer, so carefree-learn will construct the following configurations for you by default: { ..., \"trainer_config\" : { ..., \"optimizers\" : { \"all\" : { \"optimizer\" : \"adam\" , \"optimizer_config\" : { \"lr\" : 1e-3 }, \"scheduler\" : \"plateau\" , \"scheduler_config\" : { \"mode\" : \"max\" , ... } } } } } If we need to apply different optimizers on different parameters (which is quite common in GANs), we need to walk through the following two steps: Define a property in your Model which returns a list of parameters you want to optimize. Define the corresponding optimizer configs with property 's name as the dictionary key. Here's an example: from cflearn.models.base import ModelBase @ModelBase . register ( \"foo\" ) class Foo ( ModelBase ): @property def params1 ( self ): return [ self . p1 , self . p2 , ... ] @property def params2 ( self ): return [ self . p1 , self . p3 , ... ] { ..., \"trainer_config\" : { ..., \"optimizers\" : { \"params1\" : { \"optimizer\" : \"adam\" , \"optimizer_config\" : { \"lr\" : 3e-4 }, \"scheduler\" : null }, \"params2\" : { \"optimizer\" : \"nag\" , \"optimizer_config\" : { \"lr\" : 1e-3 , \"momentum\" : 0.9 }, \"scheduler\" : \"plateau\" , \"scheduler_config\" : { \"mode\" : \"max\" , ... } } } } } D eep N eural D ecision F orests \u21a9 D eep D istribution R egression \u21a9","title":"Introduction"},{"location":"introduction/#introduction","text":"","title":"Introduction"},{"location":"introduction/#advantages","text":"Like many similar projects, carefree-learn can be treated as a high-level library to help with training neural networks in PyTorch. However, carefree-learn does less and more than that. carefree-learn focuses on tabular (structured) datasets, instead of unstructured datasets (e.g. NLP datasets or CV datasets). carefree-learn provides an end-to-end pipeline on tabular datasets, including AUTOMATICALLY deal with (this part is mainly handled by carefree-data , though): Detection of redundant feature columns which can be excluded (all SAME, all DIFFERENT, etc). Detection of feature columns types (whether a feature column is string column / numerical column / categorical column). Imputation of missing values. Encoding of string columns and categorical columns (Embedding or One Hot Encoding). Pre-processing of numerical columns (Normalize, Min Max, etc.). And much more... carefree-learn can help you deal with almost ANY kind of tabular datasets, no matter how 'dirty' and 'messy' it is. It can be either trained directly with some numpy arrays, or trained indirectly with some files locate on your machine. This makes carefree-learn stand out from similar projects. carefree-learn is highly customizable for developers. We have already wrapped (almost) every single functionality / process into a single module (a Python class), and they can be replaced or enhanced either directly from source codes or from local codes with the help of some pre-defined functions provided by carefree-learn (see Registration ). carefree-learn supports easy-to-use saving and loading. By default, everything will be wrapped into a zip file! carefree-learn supports Distributed Training . Info From the discriptions above, you might notice that carefree-learn is more of a minimal Automatic Machine Learning (AutoML) solution than a pure Machine Learning package. Tip When we say ANY , it means that carefree-learn can even train on dataset with only one single sample: import cflearn toy = cflearn . make_toy_model () data = toy . tr_data . converted print ( f \"x= { data . x } , y= { data . y } \" ) # x=[[0.]], y=[[1.]] This is especially useful when we need to do unittests or to verify whether our custom modules (e.g. custom pre-processes) are correctly integrated into carefree-learn , for example: import cflearn import numpy as np # here we implement a custom processor @cflearn . register_processor ( \"plus_one\" ) class PlusOne ( cflearn . Processor ): @property def input_dim ( self ) -> int : return 1 @property def output_dim ( self ) -> int : return 1 def fit ( self , columns : np . ndarray ) -> cflearn . Processor : return self def _process ( self , columns : np . ndarray ) -> np . ndarray : return columns + 1 def _recover ( self , processed_columns : np . ndarray ) -> np . ndarray : return processed_columns - 1 # we need to specify that we use the custom process method to process our labels config = { \"data_config\" : { \"label_process_method\" : \"plus_one\" }} toy = cflearn . make_toy_model ( config ) y = toy . tr_data . converted . y processed_y = toy . tr_data . processed . y print ( f \"y= { y } , new_y= { processed_y } \" ) # y=[[1.]], new_y=[[2.]] There is one more thing we'd like to mention: carefree-learn is ' Pandas -free'. The reasons why we excluded Pandas are listed in carefree-data .","title":"Advantages"},{"location":"introduction/#design-tenets","text":"carefree-learn was designed to support most commonly used methods with 'carefree' APIs. Moreover, carefree-learn was also designed with interface which is general enough, so that more sophisticated functionality can also be easily integrated in the future. This brings a tension in how to create abstractions in code, which is a challenge for us: On the one hand, it requires a reasonably high-level abstraction so that users can easily work around with it in a standard way, without having to worry too much about the details. On the other hand, it also needs to have a very thin abstraction to allow users to do (many) other things in new ways. Breaking existing abstractions and replacing them with new ones should be fairly easy. In carefree-learn , there are three main design tenets that address this tension together: Share configurations with one single config argument (see Configurations ). Build some common blocks which shall be leveraged across different models (see Common Blocks ). Divide carefree-learn into three parts: Model , Trainer and Pipeline , each focuses on certain roles. Implemente functions ( cflearn.register_* to be exact) to ensure flexibility and control on different modules and stuffs (see Registration ). We will introduce the details in the following subsections.","title":"Design Tenets"},{"location":"introduction/#common-blocks","text":"Info Source codes path: blocks.py . Commonality is important for abstractions. When it comes to deep learning, it is not difficult to figure out the very common structure across all models: the Mapping Layers which is responsible for mapping data distrubution from one dimensional to another. Although some efforts have been made to replace the Mapping Layers (e.g. DNDF 1 ), it is undeniable that the Mapping Layers should be extracted as a stand-alone module before any other structures. But in carefree-learn , we should do more than that. Recall that carefree-learn focuses on tabular datasets, which means carefree-learn will use Mapping Layers in most cases (Unlike CNN or RNN which has Convolutional Blocks and RNNCell Blocks respectively). In this case, it is necessary to wrap multiple Mapping s into one single Module - also well known as MLP - in carefree-learn . So, in CNN we have Conv2D , in RNN we have LSTM , and in carefree-learn we have MLP .","title":"Common Blocks"},{"location":"introduction/#model","text":"Info Source codes path: base.py -> class ModelBase . In carefree-learn , a Model should implement the core algorithms. It assumes that the input data in training process is already 'batched, processed, nice and clean', but not yet 'encoded'. Fortunately, carefree-learn pre-defined some useful methods which can encode categorical columns easily. It does not care about how to train a model, it only focuses on how to make predictions with input, and how to calculate losses with them. Note Model s are likely to define MLP blocks frequently, as explained in the Common Blocks section.","title":"Model"},{"location":"introduction/#trainer","text":"Info Source codes path: core.py -> class Trainer . In carefree-learn , a Trainer should implement the high-level parts, as listed below: It assumes that the input data is already 'processed, nice and clean', but it should take care of getting input data into batches, because in real applications batching is essential for performance. It should take care of the training loop, which includes updating parameters with an optimizer, verbosing metrics, checkpointing, early stopping, logging, etc.","title":"Trainer"},{"location":"introduction/#pipeline","text":"Info Source codes path: core.py -> class Pipeline . In carefree-learn , a Pipeline should implement the preparation and API part. It should not make any assumptions to the input data, it could already be 'nice and clean', but it could also be 'dirty and messy'. Therefore, it needs to transform the original data into 'nice and clean' data and then feed it to Trainer . The data transformations include (this part is mainly handled by carefree-data , though): Imputation of missing values. Transforming string columns into categorical columns. Processing numerical columns. Processing label column (if needed). It should implement some algorithm-agnostic functions (e.g. predict , save , load , etc.).","title":"Pipeline"},{"location":"introduction/#registration","text":"Registration in carefree-learn means registering user-defined modules to carefree-learn , so carefree-learn can leverage these modules to resolve more specific tasks. In most cases, the registration stuffs are done by simply defining and updating many global dict s. For example, carefree-learn defined some useful parameter initializations in cflearn.misc.toolkit.Initializer . If we want to use our own initialization methods, simply register it and then everything will be fine: import torch import cflearn from torch.nn import Parameter initializer = cflearn . Initializer ({}) @cflearn . register_initializer ( \"all_one\" ) def all_one ( initializer_ , parameter ): parameter . fill_ ( 1. ) param = Parameter ( torch . zeros ( 3 )) with torch . no_grad (): initializer . initialize ( param , \"all_one\" ) print ( param ) # tensor([1., 1., 1.], requires_grad=True) Info Currently we mainly have 5 registrations in use: register_metric , register_optimizer , register_scheduler , register_initializer and register_processor","title":"Registration"},{"location":"introduction/#configurations","text":"In carefree-learn , we have few args and kwargs in each module. Instead, we'll use one single argument config which takes in a (shared) Python dict to configure those modules. That's why we can easily support JSON configuration, which is very useful when you need to share your models to others or reproduce others' work.","title":"Configurations"},{"location":"introduction/#scopes","text":"Since we have many stand-alone modules that provide corresponding functionalities, our configuration (which is a Python dict) will be designed 'hierarchically', and each module can read its specified configuration under its specific 'scope'. If needed, they can also access configurations defined under other 'scopes' easily because the whole configuration dict will be passed to each module. Info Currently we mainly have 6 scopes in use: root ( pipeline_config ), model_config , trainer_config , data_config , metric_config and optimizers . Suppose we have a Python dict named config now: The keys in root scope of config are those which are directly stored in the config . The keys in other scopes (e.g. data_config ) of config are those which are stored in a sub-dict, and this sub-dict is the value of the scope-name-key in config (e.g. \"data_config\" ). Here is an example: config = { # `root` scope \"foo\" : ... , \"dummy\" : ... , # `model_config` scope \"model_config\" : { \"...\" : ... }, # `trainer_config` scope \"trainer_config\" : { \"...\" : ... }, ... } We will introduce what kinds of algorithm-agnostic configurations are available in each scope and how to specify them in the Configurations section.","title":"Scopes"},{"location":"introduction/#data-loading-strategy","text":"Since carefree-learn focuses on tabular datasets, the data loading strategy is very different from unstructured datasets' strategy. For instance, it is quite common that a CV dataset is a bunch of pictures located in a folder, and we will either read them sequentially or read them in parallel. Nowadays, almost every famous deep learning framework has their own solution to load unstructured datasets efficiently, e.g. PyTorch officially implements DataLoader to support multi-process loading and other features. Although we know that RAM speed is (almost) always faster than I/O operations, we still prefer leveraging multi-process to read files than loading them all into RAM at once. This is because unstructured datasets are often too large to allocate them all to RAM. However, when it comes to tabular datasets, we prefer to load everything into RAM at the very beginning. The main reasons are listed below: Tabular datasets are often quite small and are able to put into RAM at once. Network structures for tabular datasets are often much smaller, which means using multi-process loading will cause a much heavier overhead. We need to take Distributed Training into account. If we stick to multi-process loading, there would be too many threads in the pool which is not a good practice.","title":"Data Loading Strategy"},{"location":"introduction/#distributed-training","text":"In carefree-learn , Distributed Training doesn't mean training your model on multiple GPUs or multiple machines, because carefree-learn focuses on tabular datasets which are often not as large as unstructured datasets. Instead, Distributed Training in carefree-learn means training multiple models at the same time. This is important because: Deep Learning models suffer from randomness, so we need to train multiple models with the same algorithm and calculate the mean / std of the performances to estimate the algorithm's capacity and stability. Ensembling these models (trained with the same algorithm) can boost the algorithm's performance without making any changes to the algorithm itself. Parameter searching will be easier & faster.","title":"Distributed Training"},{"location":"introduction/#terminologies","text":"In carefree-learn , there are some frequently used terminologies, and we will introduce them in this section. If you are confused by some other terminologies in carefree-learn when you are using it, feel free to edit this list:","title":"Terminologies"},{"location":"introduction/#step","text":"One step in the training process means that one mini-batch passed through our model.","title":"step"},{"location":"introduction/#epochs","text":"In most deep learning processes, training is structured into epochs. An epoch is one iteration over the entire input data, which is constructed by several step s.","title":"epochs"},{"location":"introduction/#batch_size","text":"It is a good practice to slice the data into smaller batches and iterates over these batches during training, and batch_size specifies the size of each batch. Be aware that the last batch may be smaller if the total number of samples is not divisible by the batch_size .","title":"batch_size"},{"location":"introduction/#config","text":"A config indicates the main part (or, the shared part) of the configuration.","title":"config"},{"location":"introduction/#increment_config","text":"An increment_config indicates the configurations that you want to update on config . Info This is very useful when you only want to tune a single configuration and yet you have tons of other configurations need to be fixed. In this case, you can set other configurations as config , and adjust the target configuration in increment_config .","title":"increment_config"},{"location":"introduction/#forward","text":"A forward method is a common method required by (almost) all PyTorch modules. Info Here is a nice discussion.","title":"forward"},{"location":"introduction/#task_type","text":"We use task_type = \"clf\" to indicate a classification task, and task_type = \"reg\" to indicate a regression task. Info And we'll convert them into cfdata . tabular . TaskTypes under the hood.","title":"task_type"},{"location":"introduction/#tr-cv-te","text":"In most cases, we use: x_tr , x_cv and x_te to represent training , cross validation and test features . y_tr , y_cv and y_te to represent training , cross validation and test labels .","title":"tr, cv &amp; te"},{"location":"introduction/#metrics","text":"Although losses are what we optimize directly during training, metrics are what we 'actually' want to optimize (e.g. acc , auc , f1-score , etc.). Sometimes we may want to take multiple metrics into consideration, and we may also want to eliminate the fluctuation comes with mini-batch training by applying EMA on the metrics. To make things clearer, we decided to introduce the metric_config scope (under the trainer_config scope). By default: mae & mse is used for regression tasks, while auc & acc is used for classification tasks. An EMA with decay = 0.1 will be used. Every metrics will be treated as equal. So carefree-learn will construct the following configurations for you by default (take classification tasks as an example): { ..., \"trainer_config\" : { ..., \"metric_config\" : { \"decay\" : 0.1 , \"types\" : [ \"auc\" , \"acc\" ], \"weights\" : { \"auc\" : 1.0 , \"acc\" : 1.0 } } } } It's worth mentioning that carefree-learn also supports using losses as metrics: { ..., \"trainer_config\" : { ..., \"metric_config\" : { \"decay\" : 0.1 , \"types\" : [ \"loss\" ] } } }","title":"metrics"},{"location":"introduction/#optimizers","text":"Sometimes we may want to have different optimizers to optimize different group of parameters. In order to make things easier with flexibility and control, we decided to introduce the optimizers scope (under the trainer_config scope). By default, all parameters will be optimized via one single optimizer, so carefree-learn will construct the following configurations for you by default: { ..., \"trainer_config\" : { ..., \"optimizers\" : { \"all\" : { \"optimizer\" : \"adam\" , \"optimizer_config\" : { \"lr\" : 1e-3 }, \"scheduler\" : \"plateau\" , \"scheduler_config\" : { \"mode\" : \"max\" , ... } } } } } If we need to apply different optimizers on different parameters (which is quite common in GANs), we need to walk through the following two steps: Define a property in your Model which returns a list of parameters you want to optimize. Define the corresponding optimizer configs with property 's name as the dictionary key. Here's an example: from cflearn.models.base import ModelBase @ModelBase . register ( \"foo\" ) class Foo ( ModelBase ): @property def params1 ( self ): return [ self . p1 , self . p2 , ... ] @property def params2 ( self ): return [ self . p1 , self . p3 , ... ] { ..., \"trainer_config\" : { ..., \"optimizers\" : { \"params1\" : { \"optimizer\" : \"adam\" , \"optimizer_config\" : { \"lr\" : 3e-4 }, \"scheduler\" : null }, \"params2\" : { \"optimizer\" : \"nag\" , \"optimizer_config\" : { \"lr\" : 1e-3 , \"momentum\" : 0.9 }, \"scheduler\" : \"plateau\" , \"scheduler_config\" : { \"mode\" : \"max\" , ... } } } } } D eep N eural D ecision F orests \u21a9 D eep D istribution R egression \u21a9","title":"optimizers"},{"location":"about/license/","text":"License \u00b6 carefree-learn License (MIT) \u00b6 Copyright \u00a9 2020 carefree0910 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/license/#license","text":"","title":"License"},{"location":"about/license/#carefree-learn-license-mit","text":"Copyright \u00a9 2020 carefree0910 Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"carefree-learn License (MIT)"},{"location":"about/release-notes/","text":"Release Notes \u00b6 Maintenance team \u00b6 Currently there is only one member from China. @carefree0910","title":"Release Notes"},{"location":"about/release-notes/#release-notes","text":"","title":"Release Notes"},{"location":"about/release-notes/#maintenance-team","text":"Currently there is only one member from China. @carefree0910","title":"Maintenance team"},{"location":"user-guide/configurations/","text":"Configurations \u00b6 Although it is possible to get a rather good performance with default configurations, performance might be gained easily by specifying configurations with our prior knowledges. We've mentioned the basic ideas on how to configure carefree-learn in Introduction , so we will focus on introducing how to actually do it in this page. Info Notice that configurations listed in this page are algorithm-agnostic. For reference on concepts repeated across the configurations, see Terminologies . Design Tenets \u00b6 There are several common practices for specifying configurations. In many high-level Machine Learning modules (e.g. scikit-learn ), configurations are directly specified by using args and kwargs to instantiate an object of corresponding algorithm. In carefree-learn , however, since we've wrapped many procedures together (in Pipeline ) to provide a more 'carefree' usage, we cannot put all those configurations in the definition of the class because that will be too long and too messy. Instead, we will use one single, shared dict to specify configurations. There are several advantages by doing so, as listed below: It's much more flexible and easier to extend. It's much easier to reproduce other's work, because a single JSON file will be enough. It's much easier to share configurations between different modules. This is especially helpful in carefree-learn because we've tried hard to do elegant abstractions, which lead us to implement many individual modules to handle different problems. In this case, some 'global' information will be hard to access if we don't share configurations. It's possible to define high level APIs which accept args and kwargs for easier usages (e.g. cflearn.make ). How to Specify Configurations \u00b6 There are two ways to specify configurations in carefree-learn : directly with a Python dict or indirectly with a JSON file. Configure with Python dict \u00b6 import cflearn # specify any configurations config = { \"foo\" : 0 , \"dummy\" : 1 } fcnn = cflearn . make ( ** config ) print ( fcnn . config ) # {\"foo\": 0, \"dummy\": 1, ...} Configure with JSON file \u00b6 In order to use JSON file as configuration, suppose you want to run my_script.py , and it contains the following codes: import cflearn config = \"./configs/basic.json\" increment_config = { \"foo\" : 2 } fcnn = cflearn . make ( config = config , increment_config = increment_config ) Since config is set to \"./configs/basic.json\" , the file structure should be: -- my_script.py -- configs |-- basic.json Suppose basic.json contains following stuffs: { \"foo\" : 0 , \"dummy\" : 1 } Then the output of print ( fcnn . config ) should be: { \"foo\" : 2 , \"dummy\" : 1 , ... } It is OK to get rid of increment_config , in which case the configuration will be completely controlled by basic.json : import cflearn config = \"./configs/basic.json\" fcnn = cflearn . make ( config = config ) print ( fcnn . config ) # {\"foo\": 0, \"dummy\": 1, ...} High Level API \u00b6 In order to provide out of the box tools, carefree-learn implements high level APIs for training, estimating, distributed, HPO, etc. In this section we'll introduce cflearn.make , because it contains most of the frequently used configurations, and other APIs' arguments depend on it more or less. def make ( model : str = \"fcnn\" , * , config : general_config_type = None , increment_config : general_config_type = None , delim : Optional [ str ] = None , task_type : Optional [ str ] = None , skip_first : Optional [ bool ] = None , cv_split : Optional [ Union [ float , int ]] = None , min_epoch : Optional [ int ] = None , num_epoch : Optional [ int ] = None , max_epoch : Optional [ int ] = None , batch_size : Optional [ int ] = None , max_snapshot_num : Optional [ int ] = None , clip_norm : Optional [ float ] = None , ema_decay : Optional [ float ] = None , ts_config : Optional [ TimeSeriesConfig ] = None , aggregation : Optional [ str ] = None , aggregation_config : Optional [ Dict [ str , Any ]] = None , ts_label_collator_config : Optional [ Dict [ str , Any ]] = None , data_config : Optional [ Dict [ str , Any ]] = None , read_config : Optional [ Dict [ str , Any ]] = None , model_config : Optional [ Dict [ str , Any ]] = None , metrics : Optional [ Union [ str , List [ str ]]] = None , metric_config : Optional [ Dict [ str , Any ]] = None , optimizer : Optional [ str ] = None , scheduler : Optional [ str ] = None , optimizer_config : Optional [ Dict [ str , Any ]] = None , scheduler_config : Optional [ Dict [ str , Any ]] = None , optimizers : Optional [ Dict [ str , Any ]] = None , logging_file : Optional [ str ] = None , logging_folder : Optional [ str ] = None , trigger_logging : Optional [ bool ] = None , trial : Optional [ Trial ] = None , tracker_config : Optional [ Dict [ str , Any ]] = None , cuda : Optional [ Union [ int , str ]] = None , verbose_level : Optional [ int ] = None , use_timing_context : Optional [ bool ] = None , use_tqdm : Optional [ bool ] = None , ** kwargs : Any , ) -> Pipeline model [default = \"fcnn\" ] Specify which model we're going to use. Currently carefree-learn supports: \"fcnn\" , \"nnb\" , \"ndt\" , \"tree_dnn\" and \"ddr\" for basic usages. \"rnn\" and \"transformer\" for time series usages. config [default = None ] Specify the configuration. increment_config [default = None ] Specify the increment configuration. delim [default = None ] Specify the delimiter of the dataset file. Only take effects when we are using file datasets. task_type [default = None ] Specify the task type. skip_first [default = None ] Specify whether the first row of the dataset file should be skipped. Only take effects when we are using file datasets. cv_split [default = None ] Specify the split of the cross validation dataset. If cv_split < 1 , it will be the 'ratio' comparing to the whole dataset. If cv_split > 1 , it will be the exact 'size'. If cv_split == 1 , cv_split == \"ratio\" if isinstance ( cv_split , float ) else \"size\" min_epoch [default = None ] Specify the minimum number of epoch. num_epoch [default = None ] Specify number of epoch. Notice that in most cases this will not be the final epoch number. max_epoch [default = None ] Specify the maximum number of epoch. batch_size [default = None ] Specify the number of samples in each batch. max_snapshot_num [default = None ] Specify the maximum number of checkpoint files we could save during training. clip_norm [default = None ] Given a gradient g , and the clip_norm value, we will normalize g so that its L2-norm is less than or equal to clip_norm . If 0. , then no gradient clip will be performed. ema_decay [default = None ] When training a model, it is often beneficial to maintain E xponential M oving A verages with a certain decay rate ( ema_decay ) of the trained parameters. Evaluations that use averaged parameters sometimes produce significantly better results than the final trained values. If 0. , then no EMA will be used. ts_config [default = None ] Specify the time series config (experimental). aggregation [default = None ] Specify the aggregation used in time series tasks (experimental). aggregation_config [default = None ] Specify the configuration of aggregation used in time series tasks (experimental). ts_label_collator_config [default = None ] Specify the configuration of the label collator used in time series tasks (experimental). data_config [default = None ] kwargs used in cfdata.tabular.TabularData . read_config [default = None ] kwargs used in cfdata.tabular.TabularData.read . model_config [default = None ] Configurations used in Model . metrics [default = None ] Specify which metric(s) are we going to use to monitor our training process metric_config [default = None ] Specify the fine grained configurations of metrics. See metrics for more details. optimizer [default = None ] Specify which optimizer will be used. scheduler [default = None ] Specify which learning rate scheduler will be used. optimizer_config [default = None ] Specify optimizer's configuration. scheduler_config [default = None ] Specify scheduler's configuration. optimizers [default = None ] Specify the fine grained configurations of optimizers and schedulers. See optimizers for more details. logging_file [default = None ] Specify the logging file. logging_folder [default = None ] Specify the logging folder. trigger_logging [default = None ] Whether log messages into a log file. trial [default = None ] optuna.trial.Trial , should not be set manually because this argument should only be set in cflearn.optuna_tune internally. tracker_config [default = None ] Specify the configuration of cftool.ml.Tracker . If None , then Tracker will not be used. cuda [default = None ] Specify the working GPU. verbose_level [default = None ] Specify the verbose level. use_timing_context [default = None ] Whether utilize the timing_context or not. use_tqdm [default = None ] Whether utilize the tqdm progress bar or not. kwargs [default = {} ] Other configurations.","title":"Configurations"},{"location":"user-guide/configurations/#configurations","text":"Although it is possible to get a rather good performance with default configurations, performance might be gained easily by specifying configurations with our prior knowledges. We've mentioned the basic ideas on how to configure carefree-learn in Introduction , so we will focus on introducing how to actually do it in this page. Info Notice that configurations listed in this page are algorithm-agnostic. For reference on concepts repeated across the configurations, see Terminologies .","title":"Configurations"},{"location":"user-guide/configurations/#design-tenets","text":"There are several common practices for specifying configurations. In many high-level Machine Learning modules (e.g. scikit-learn ), configurations are directly specified by using args and kwargs to instantiate an object of corresponding algorithm. In carefree-learn , however, since we've wrapped many procedures together (in Pipeline ) to provide a more 'carefree' usage, we cannot put all those configurations in the definition of the class because that will be too long and too messy. Instead, we will use one single, shared dict to specify configurations. There are several advantages by doing so, as listed below: It's much more flexible and easier to extend. It's much easier to reproduce other's work, because a single JSON file will be enough. It's much easier to share configurations between different modules. This is especially helpful in carefree-learn because we've tried hard to do elegant abstractions, which lead us to implement many individual modules to handle different problems. In this case, some 'global' information will be hard to access if we don't share configurations. It's possible to define high level APIs which accept args and kwargs for easier usages (e.g. cflearn.make ).","title":"Design Tenets"},{"location":"user-guide/configurations/#how-to-specify-configurations","text":"There are two ways to specify configurations in carefree-learn : directly with a Python dict or indirectly with a JSON file.","title":"How to Specify Configurations"},{"location":"user-guide/configurations/#configure-with-python-dict","text":"import cflearn # specify any configurations config = { \"foo\" : 0 , \"dummy\" : 1 } fcnn = cflearn . make ( ** config ) print ( fcnn . config ) # {\"foo\": 0, \"dummy\": 1, ...}","title":"Configure with Python dict"},{"location":"user-guide/configurations/#configure-with-json-file","text":"In order to use JSON file as configuration, suppose you want to run my_script.py , and it contains the following codes: import cflearn config = \"./configs/basic.json\" increment_config = { \"foo\" : 2 } fcnn = cflearn . make ( config = config , increment_config = increment_config ) Since config is set to \"./configs/basic.json\" , the file structure should be: -- my_script.py -- configs |-- basic.json Suppose basic.json contains following stuffs: { \"foo\" : 0 , \"dummy\" : 1 } Then the output of print ( fcnn . config ) should be: { \"foo\" : 2 , \"dummy\" : 1 , ... } It is OK to get rid of increment_config , in which case the configuration will be completely controlled by basic.json : import cflearn config = \"./configs/basic.json\" fcnn = cflearn . make ( config = config ) print ( fcnn . config ) # {\"foo\": 0, \"dummy\": 1, ...}","title":"Configure with JSON file"},{"location":"user-guide/configurations/#high-level-api","text":"In order to provide out of the box tools, carefree-learn implements high level APIs for training, estimating, distributed, HPO, etc. In this section we'll introduce cflearn.make , because it contains most of the frequently used configurations, and other APIs' arguments depend on it more or less. def make ( model : str = \"fcnn\" , * , config : general_config_type = None , increment_config : general_config_type = None , delim : Optional [ str ] = None , task_type : Optional [ str ] = None , skip_first : Optional [ bool ] = None , cv_split : Optional [ Union [ float , int ]] = None , min_epoch : Optional [ int ] = None , num_epoch : Optional [ int ] = None , max_epoch : Optional [ int ] = None , batch_size : Optional [ int ] = None , max_snapshot_num : Optional [ int ] = None , clip_norm : Optional [ float ] = None , ema_decay : Optional [ float ] = None , ts_config : Optional [ TimeSeriesConfig ] = None , aggregation : Optional [ str ] = None , aggregation_config : Optional [ Dict [ str , Any ]] = None , ts_label_collator_config : Optional [ Dict [ str , Any ]] = None , data_config : Optional [ Dict [ str , Any ]] = None , read_config : Optional [ Dict [ str , Any ]] = None , model_config : Optional [ Dict [ str , Any ]] = None , metrics : Optional [ Union [ str , List [ str ]]] = None , metric_config : Optional [ Dict [ str , Any ]] = None , optimizer : Optional [ str ] = None , scheduler : Optional [ str ] = None , optimizer_config : Optional [ Dict [ str , Any ]] = None , scheduler_config : Optional [ Dict [ str , Any ]] = None , optimizers : Optional [ Dict [ str , Any ]] = None , logging_file : Optional [ str ] = None , logging_folder : Optional [ str ] = None , trigger_logging : Optional [ bool ] = None , trial : Optional [ Trial ] = None , tracker_config : Optional [ Dict [ str , Any ]] = None , cuda : Optional [ Union [ int , str ]] = None , verbose_level : Optional [ int ] = None , use_timing_context : Optional [ bool ] = None , use_tqdm : Optional [ bool ] = None , ** kwargs : Any , ) -> Pipeline model [default = \"fcnn\" ] Specify which model we're going to use. Currently carefree-learn supports: \"fcnn\" , \"nnb\" , \"ndt\" , \"tree_dnn\" and \"ddr\" for basic usages. \"rnn\" and \"transformer\" for time series usages. config [default = None ] Specify the configuration. increment_config [default = None ] Specify the increment configuration. delim [default = None ] Specify the delimiter of the dataset file. Only take effects when we are using file datasets. task_type [default = None ] Specify the task type. skip_first [default = None ] Specify whether the first row of the dataset file should be skipped. Only take effects when we are using file datasets. cv_split [default = None ] Specify the split of the cross validation dataset. If cv_split < 1 , it will be the 'ratio' comparing to the whole dataset. If cv_split > 1 , it will be the exact 'size'. If cv_split == 1 , cv_split == \"ratio\" if isinstance ( cv_split , float ) else \"size\" min_epoch [default = None ] Specify the minimum number of epoch. num_epoch [default = None ] Specify number of epoch. Notice that in most cases this will not be the final epoch number. max_epoch [default = None ] Specify the maximum number of epoch. batch_size [default = None ] Specify the number of samples in each batch. max_snapshot_num [default = None ] Specify the maximum number of checkpoint files we could save during training. clip_norm [default = None ] Given a gradient g , and the clip_norm value, we will normalize g so that its L2-norm is less than or equal to clip_norm . If 0. , then no gradient clip will be performed. ema_decay [default = None ] When training a model, it is often beneficial to maintain E xponential M oving A verages with a certain decay rate ( ema_decay ) of the trained parameters. Evaluations that use averaged parameters sometimes produce significantly better results than the final trained values. If 0. , then no EMA will be used. ts_config [default = None ] Specify the time series config (experimental). aggregation [default = None ] Specify the aggregation used in time series tasks (experimental). aggregation_config [default = None ] Specify the configuration of aggregation used in time series tasks (experimental). ts_label_collator_config [default = None ] Specify the configuration of the label collator used in time series tasks (experimental). data_config [default = None ] kwargs used in cfdata.tabular.TabularData . read_config [default = None ] kwargs used in cfdata.tabular.TabularData.read . model_config [default = None ] Configurations used in Model . metrics [default = None ] Specify which metric(s) are we going to use to monitor our training process metric_config [default = None ] Specify the fine grained configurations of metrics. See metrics for more details. optimizer [default = None ] Specify which optimizer will be used. scheduler [default = None ] Specify which learning rate scheduler will be used. optimizer_config [default = None ] Specify optimizer's configuration. scheduler_config [default = None ] Specify scheduler's configuration. optimizers [default = None ] Specify the fine grained configurations of optimizers and schedulers. See optimizers for more details. logging_file [default = None ] Specify the logging file. logging_folder [default = None ] Specify the logging folder. trigger_logging [default = None ] Whether log messages into a log file. trial [default = None ] optuna.trial.Trial , should not be set manually because this argument should only be set in cflearn.optuna_tune internally. tracker_config [default = None ] Specify the configuration of cftool.ml.Tracker . If None , then Tracker will not be used. cuda [default = None ] Specify the working GPU. verbose_level [default = None ] Specify the verbose level. use_timing_context [default = None ] Whether utilize the timing_context or not. use_tqdm [default = None ] Whether utilize the tqdm progress bar or not. kwargs [default = {} ] Other configurations.","title":"High Level API"}]}