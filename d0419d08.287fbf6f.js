(window.webpackJsonp=window.webpackJsonp||[]).push([[25],{105:function(e,t,n){"use strict";n.d(t,"a",(function(){return m})),n.d(t,"b",(function(){return d}));var a=n(0),r=n.n(a);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function c(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?c(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):c(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=r.a.createContext({}),p=function(e){var t=r.a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},m=function(e){var t=p(e.components);return r.a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},b=r.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),m=p(n),b=a,d=m["".concat(c,".").concat(b)]||m[b]||u[b]||i;return n?r.a.createElement(d,o(o({ref:t},l),{},{components:n})):r.a.createElement(d,o({ref:t},l))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,c=new Array(i);c[0]=b;var o={};for(var s in t)hasOwnProperty.call(t,s)&&(o[s]=t[s]);o.originalType=e,o.mdxType="string"==typeof e?e:a,c[1]=o;for(var l=2;l<i;l++)c[l]=n[l];return r.a.createElement.apply(null,c)}return r.a.createElement.apply(null,n)}b.displayName="MDXCreateElement"},95:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return c})),n.d(t,"metadata",(function(){return o})),n.d(t,"rightToc",(function(){return s})),n.d(t,"default",(function(){return p}));var a=n(3),r=n(7),i=(n(0),n(105)),c={id:"distributed",title:"Distributed"},o={unversionedId:"user-guides/distributed",id:"user-guides/distributed",isDocsHomePage:!1,title:"Distributed",description:"Distributed Training",source:"@site/docs/user-guides/distributed.md",slug:"/user-guides/distributed",permalink:"/carefree-learn-doc/docs/user-guides/distributed",version:"current",lastUpdatedAt:1605876656,sidebar:"docs",previous:{title:"AutoML",permalink:"/carefree-learn-doc/docs/user-guides/auto-ml"},next:{title:"Production",permalink:"/carefree-learn-doc/docs/user-guides/production"}},s=[{value:"Distributed Training",id:"distributed-training",children:[]},{value:"Hyper Parameter Optimization (HPO)",id:"hyper-parameter-optimization-hpo",children:[]}],l={rightToc:s};function p(e){var t=e.components,n=Object(r.a)(e,["components"]);return Object(i.b)("wrapper",Object(a.a)({},l,n,{components:t,mdxType:"MDXLayout"}),Object(i.b)("h2",{id:"distributed-training"},"Distributed Training"),Object(i.b)("p",null,"In ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn"),", ",Object(i.b)("strong",{parentName:"p"},"Distributed Training")," doesn't mean training your model on multiple GPUs or multiple machines, because ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," focuses on tabular datasets (or, structured datasets) which are often not as large as unstructured datasets. Instead, ",Object(i.b)("strong",{parentName:"p"},"Distributed Training")," in ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," means ",Object(i.b)("strong",{parentName:"p"},"training multiple models")," at the same time. This is important because:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"Deep Learning models suffer from randomness, so we need to train multiple models with the same algorithm and calculate the mean / std of the performances to evaluate the algorithm's capacity and stability."),Object(i.b)("li",{parentName:"ul"},"Ensemble these models (which are trained with the same algorithm) can boost the algorithm's performance without making any changes to the algorithm itself."),Object(i.b)("li",{parentName:"ul"},"Parameter searching will be easier & faster.")),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),'import cflearn\nfrom cfdata.tabular import TabularDataset\n\n# It is necessary to wrap codes under \'__main__\' on WINDOWS platform when running distributed codes\nif __name__ == \'__main__\':\n    x, y = TabularDataset.iris().xy\n    # Notice that 3 fcnn were trained simultaneously with this line of code\n    results = cflearn.repeat_with(x, y, num_repeat=3, num_jobs=0)\n    patterns = results.patterns["fcnn"]\n    # And it is fairly straight forward to apply stacking ensemble\n    ensemble = cflearn.Ensemble.stacking(patterns)\n    patterns_dict = {"fcnn_3": patterns, "fcnn_3_ensemble": ensemble}\n    cflearn.evaluate(x, y, metrics=["acc", "auc"], other_patterns=patterns_dict)\n')),Object(i.b)("p",null,"Then you will see something like this:"),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-text"}),"================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|         fcnn_3         |    0.937778    |    0.017498    |    0.920280    | -- 0.993911 -- |    0.000274    |    0.993637    |\n--------------------------------------------------------------------------------------------------------------------------------\n|    fcnn_3_ensemble     | -- 0.953333 -- | -- 0.000000 -- | -- 0.953333 -- |    0.993867    | -- 0.000000 -- | -- 0.993867 -- |\n================================================================================================================================\n")),Object(i.b)("div",{className:"admonition admonition-note alert alert--secondary"},Object(i.b)("div",Object(a.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(a.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(a.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(a.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})))),"note")),Object(i.b)("div",Object(a.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"You might notice that the best results of each column is 'highlighted' with a pair of '--'."))),Object(i.b)("h2",{id:"hyper-parameter-optimization-hpo"},"Hyper Parameter Optimization (HPO)"),Object(i.b)("p",null,"Although ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," has already provided an ",Object(i.b)("a",Object(a.a)({parentName:"p"},{href:"auto-ml"}),Object(i.b)("inlineCode",{parentName:"a"},"AutoML"))," API, we can still play with the ",Object(i.b)("strong",{parentName:"p"},"HPO")," APIs manually:"),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),"import cflearn\nfrom cfdata.tabular import TabularDataset\n \nif __name__ == '__main__':\n    x, y = TabularDataset.iris().xy\n    # Bayesian Optimization (BO) will be used as default\n    hpo = cflearn.tune_with(\n        x, y,\n        task_type=\"clf\",\n        num_repeat=2, num_parallel=0, num_search=10\n    )\n    # We can further train our model with the best hyper-parameters we've obtained:\n    m = cflearn.make(**hpo.best_param).fit(x, y)\n    cflearn.evaluate(x, y, pipelines=m)\n")),Object(i.b)("p",null,"Then you will see something like this:"),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-text"}),"~~~  [ info ] Results\n================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|        0659e09f        |    0.943333    |    0.016667    |    0.926667    |    0.995500    |    0.001967    |    0.993533    |\n--------------------------------------------------------------------------------------------------------------------------------\n|        08a0a030        |    0.796667    |    0.130000    |    0.666667    |    0.969333    |    0.012000    |    0.957333    |\n--------------------------------------------------------------------------------------------------------------------------------\n|        1962285c        |    0.950000    |    0.003333    |    0.946667    |    0.997467    |    0.000533    |    0.996933    |\n--------------------------------------------------------------------------------------------------------------------------------\n|        1eb7f2a0        |    0.933333    |    0.020000    |    0.913333    |    0.994833    |    0.003033    |    0.991800    |\n--------------------------------------------------------------------------------------------------------------------------------\n|        4ed5bb3b        |    0.973333    |    0.013333    |    0.960000    |    0.998733    |    0.000467    |    0.998267    |\n--------------------------------------------------------------------------------------------------------------------------------\n|        5a652f3c        |    0.953333    | -- 0.000000 -- |    0.953333    |    0.997400    |    0.000133    |    0.997267    |\n--------------------------------------------------------------------------------------------------------------------------------\n|        82c35e77        |    0.940000    |    0.020000    |    0.920000    |    0.995467    |    0.002133    |    0.993333    |\n--------------------------------------------------------------------------------------------------------------------------------\n|        a9ef52d0        | -- 0.986667 -- |    0.006667    | -- 0.980000 -- | -- 0.999200 -- | -- 0.000000 -- | -- 0.999200 -- |\n--------------------------------------------------------------------------------------------------------------------------------\n|        ba2e179a        |    0.946667    |    0.026667    |    0.920000    |    0.995633    |    0.001900    |    0.993733    |\n--------------------------------------------------------------------------------------------------------------------------------\n|        ec8c0837        |    0.973333    | -- 0.000000 -- |    0.973333    |    0.998867    |    0.000067    |    0.998800    |\n================================================================================================================================\n\n~~~  [ info ] Best Parameters\n----------------------------------------------------------------------------------------------------\nacc  (a9ef52d0) (0.986667 \xb1 0.006667)\n----------------------------------------------------------------------------------------------------\n{'optimizer': 'rmsprop', 'optimizer_config': {'lr': 0.005810863965757382}}\n----------------------------------------------------------------------------------------------------\nauc  (a9ef52d0) (0.999200 \xb1 0.000000)\n----------------------------------------------------------------------------------------------------\n{'optimizer': 'rmsprop', 'optimizer_config': {'lr': 0.005810863965757382}}\n----------------------------------------------------------------------------------------------------\nbest (a9ef52d0)\n----------------------------------------------------------------------------------------------------\n{'optimizer': 'rmsprop', 'optimizer_config': {'lr': 0.005810863965757382}}\n----------------------------------------------------------------------------------------------------\n\n~~  [ info ] Results\n================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|          fcnn          |    0.980000    |    0.000000    |    0.980000    |    0.998867    |    0.000000    |    0.998867    |\n================================================================================================================================\n")),Object(i.b)("p",null,"You might notice that:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"The final results obtained by ",Object(i.b)("strong",{parentName:"li"},"HPO")," is even better than the stacking ensemble results mentioned above."),Object(i.b)("li",{parentName:"ul"},"We search for ",Object(i.b)("inlineCode",{parentName:"li"},"optimizer")," and ",Object(i.b)("inlineCode",{parentName:"li"},"lr")," as default. In fact, we can manually passed ",Object(i.b)("inlineCode",{parentName:"li"},"params")," into ",Object(i.b)("inlineCode",{parentName:"li"},"cflearn.tune_with"),". If not, then ",Object(i.b)("inlineCode",{parentName:"li"},"carefree-learn")," will execute following codes:")),Object(i.b)("pre",null,Object(i.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),'from cftool.ml.param_utils import *\n\nparams = {\n    "optimizer": String(Choice(values=["sgd", "rmsprop", "adam"])),\n    "optimizer_config": {\n        "lr": Float(Exponential(1e-5, 0.1))\n    }\n}\n')))}p.isMDXComponent=!0}}]);