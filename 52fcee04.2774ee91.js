(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{108:function(e,t,a){"use strict";a.d(t,"a",(function(){return p})),a.d(t,"b",(function(){return u}));var n=a(0),r=a.n(n);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function c(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function s(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?c(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):c(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function o(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=r.a.createContext({}),b=function(e){var t=r.a.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):s(s({},t),e)),a},p=function(e){var t=b(e.components);return r.a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},m=r.a.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,c=e.parentName,l=o(e,["components","mdxType","originalType","parentName"]),p=b(a),m=n,u=p["".concat(c,".").concat(m)]||p[m]||d[m]||i;return a?r.a.createElement(u,s(s({ref:t},l),{},{components:a})):r.a.createElement(u,s({ref:t},l))}));function u(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,c=new Array(i);c[0]=m;var s={};for(var o in t)hasOwnProperty.call(t,o)&&(s[o]=t[o]);s.originalType=e,s.mdxType="string"==typeof e?e:n,c[1]=s;for(var l=2;l<i;l++)c[l]=a[l];return r.a.createElement.apply(null,c)}return r.a.createElement.apply(null,a)}m.displayName="MDXCreateElement"},83:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return c})),a.d(t,"metadata",(function(){return s})),a.d(t,"rightToc",(function(){return o})),a.d(t,"default",(function(){return b}));var n=a(3),r=a(7),i=(a(0),a(108)),c={id:"Iris",title:"Iris"},s={unversionedId:"examples/Iris",id:"examples/Iris",isDocsHomePage:!1,title:"Iris",description:"| Python source code | Jupyter Notebook |",source:"@site/docs/examples/iris.md",slug:"/examples/Iris",permalink:"/carefree-learn-doc/docs/examples/Iris",version:"current",lastUpdatedAt:1607794362,sidebar:"docs",previous:{title:"Configurations",permalink:"/carefree-learn-doc/docs/getting-started/configurations"},next:{title:"Titanic",permalink:"/carefree-learn-doc/docs/examples/Titanic"}},o=[{value:"Basic Usages",id:"basic-usages",children:[]},{value:"Benchmarking",id:"benchmarking",children:[]},{value:"Advanced Benchmarking",id:"advanced-benchmarking",children:[]},{value:"AutoML on\xa0Iris",id:"automl-on-iris",children:[]},{value:"Conclusion",id:"conclusion",children:[]}],l={rightToc:o};function b(e){var t=e.components,a=Object(r.a)(e,["components"]);return Object(i.b)("wrapper",Object(n.a)({},l,a,{components:t,mdxType:"MDXLayout"}),Object(i.b)("table",null,Object(i.b)("thead",{parentName:"table"},Object(i.b)("tr",{parentName:"thead"},Object(i.b)("th",Object(n.a)({parentName:"tr"},{align:"center"}),"Python source code"),Object(i.b)("th",Object(n.a)({parentName:"tr"},{align:"center"}),"Jupyter Notebook"))),Object(i.b)("tbody",{parentName:"table"},Object(i.b)("tr",{parentName:"tbody"},Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),Object(i.b)("a",Object(n.a)({parentName:"td"},{href:"https://github.com/carefree0910/carefree-learn/blob/dev/examples/iris/iris.py"}),"iris.py")),Object(i.b)("td",Object(n.a)({parentName:"tr"},{align:"center"}),Object(i.b)("a",Object(n.a)({parentName:"td"},{href:"https://github.com/carefree0910/carefree-learn/blob/dev/examples/iris/iris.ipynb"}),"iris.ipynb"))))),Object(i.b)("p",null,"Here are some of the information provided by the official website:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"This is perhaps the best known database to be found in the pattern recognition literature.\nThe data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.\nPredicted attribute: class of iris plant.\n")),Object(i.b)("p",null,"And here's the pandas-view of the raw data:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"      f0   f1   f2   f3           label\n0    5.1  3.5  1.4  0.2     Iris-setosa\n1    4.9  3.0  1.4  0.2     Iris-setosa\n2    4.7  3.2  1.3  0.2     Iris-setosa\n3    4.6  3.1  1.5  0.2     Iris-setosa\n4    5.0  3.6  1.4  0.2     Iris-setosa\n..   ...  ...  ...  ...             ...\n145  6.7  3.0  5.2  2.3  Iris-virginica\n146  6.3  2.5  5.0  1.9  Iris-virginica\n147  6.5  3.0  5.2  2.0  Iris-virginica\n148  6.2  3.4  5.4  2.3  Iris-virginica\n149  5.9  3.0  5.1  1.8  Iris-virginica\n\n[150 rows x 5 columns]\n")),Object(i.b)("div",{className:"admonition admonition-note alert alert--secondary"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})))),"note")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("ul",{parentName:"div"},Object(i.b)("li",{parentName:"ul"},"You can download the raw data (",Object(i.b)("inlineCode",{parentName:"li"},"iris.data"),") with ",Object(i.b)("a",Object(n.a)({parentName:"li"},{href:"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"}),"this link"),"."),Object(i.b)("li",{parentName:"ul"},"We didn't use pandas in our code, but it is convenient to visualize some data with it though \ud83e\udd23")))),Object(i.b)("h2",{id:"basic-usages"},"Basic Usages"),Object(i.b)("p",null,"Traditionally, we need to process the raw data before we feed them into our machine learning models (e.g. encode the label column, which is a string column, into an ordinal column). In ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn"),", however, we can train neural networks directly on files without worrying about the rest:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'import cflearn\n\nm = cflearn.make().fit("iris.data")\n')),Object(i.b)("p",null,"What's going under the hood is that ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," will try to parse the ",Object(i.b)("inlineCode",{parentName:"p"},"iris.data")," automatically (with the help of ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/carefree0910/carefree-data"}),"carefree-data"),"), split the data into training set and validation set, with which we'll train a fully connected neural network (fcnn)."),Object(i.b)("p",null,"We can further inspect the processed data if we want to know how ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," actually parsed the input data:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),"print(m.tr_data.raw.x[0])\nprint(m.tr_data.raw.y[0])\nprint(m.tr_data.processed.x[0])\nprint(m.tr_data.processed.y[0])\n")),Object(i.b)("p",null,"Which yields"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"['4.6', '3.6', '1.0', '0.2']\n['Iris-setosa']\n[-1.5065205  1.2634597 -1.5687355 -1.3129768]\n[0]\n")),Object(i.b)("p",null,"It shows that the raw data is carefully normalized into numerical data that neural networks can accept. You may also notice that the first elements are not identical with the first line of the raw data, this is caused by the auto-shuffle mechanism introduced in ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/carefree0910/carefree-data"}),"carefree-data"),"."),Object(i.b)("p",null,"What's more, by saying ",Object(i.b)("em",{parentName:"p"},"normalized"),", it means that the input features will be automatically normalized to ",Object(i.b)("inlineCode",{parentName:"p"},"mean=0.0")," and ",Object(i.b)("inlineCode",{parentName:"p"},"std=1.0"),":"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),"import numpy as np\n\ntr_x = m.tr_data.processed.x\ncv_x = m.cv_data.processed.x\nstacked = np.vstack([tr_x, cv_x])\nprint(stacked.mean(0))\nprint(stacked.std(0))\n")),Object(i.b)("p",null,"Which yields"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"[ 3.1739475e-08 -3.7471455e-07 -1.9907951e-07 -8.0267590e-08]\n[0.99999976 0.9999997  1.0000002  0.9999999 ]\n")),Object(i.b)("div",{className:"admonition admonition-info alert alert--info"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"})))),"info")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"The results shown above means we first normalized the data before we actually split it into train & validation set."))),Object(i.b)("p",null,"After training on files, ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," can predict & evaluate on files directly as well. We'll handle the data parsing and normalization for you automatically:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'# `contains_labels` is set to True because `iris.data` itself contains labels\npredictions = m.predict("iris.data", contains_labels=True)\n# evaluations could be achieved easily with cflearn.evaluate\ncflearn.evaluate("iris.data", pipelines=m)\n')),Object(i.b)("p",null,"Which yields"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|          fcnn          |    0.926667    |    0.000000    |    0.926667    |    0.994800    |    0.000000    |    0.994800    |\n================================================================================================================================\n")),Object(i.b)("h2",{id:"benchmarking"},"Benchmarking"),Object(i.b)("p",null,"As we know, neural networks are trained with ",Object(i.b)("strong",{parentName:"p"},Object(i.b)("em",{parentName:"strong"},"stochastic"))," gradient descent (and its variants), which will introduce some randomness to the final result, even if we are training on the same dataset. In this case, we need to repeat the same task several times in order to obtain the bias & variance of our neural networks. Fortunately, ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," introduced ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"../user-guides/distributed#repeat_with"}),Object(i.b)("inlineCode",{parentName:"a"},"repeat_with"))," API, which can achieve this goal easily with only a few lines of code:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'# With num_repeat=3 specified, we\'ll train 3 models on `iris.data`.\nresult = cflearn.repeat_with("iris.data", num_repeat=3)\ncflearn.evaluate("iris.data", pipelines=result.pipelines)\n')),Object(i.b)("p",null,"Which yields"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|          fcnn          |    0.902222    |    0.019116    |    0.883106    |    0.985778    |    0.004722    |    0.981055    |\n================================================================================================================================\n")),Object(i.b)("p",null,"We can also compare the performances across different models:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'# With models=["linear", "fcnn"], we\'ll train both linear models and fcnn models.\nmodels = ["linear", "fcnn"]\nresult = cflearn.repeat_with("iris.data", models=models, num_repeat=3)\ncflearn.evaluate("iris.data", pipelines=result.pipelines)\n')),Object(i.b)("p",null,"Which yields"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|          fcnn          | -- 0.915556 -- | -- 0.027933 -- | -- 0.887623 -- | -- 0.985467 -- | -- 0.004121 -- | -- 0.981345 -- |\n--------------------------------------------------------------------------------------------------------------------------------\n|         linear         |    0.620000    |    0.176970    |    0.443030    |    0.733778    |    0.148427    |    0.585351    |\n================================================================================================================================\n")),Object(i.b)("p",null,"It is worth mentioning that ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," supports ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"../user-guides/distributed#distributed-training"}),Object(i.b)("inlineCode",{parentName:"a"},"Distributed Training")),", which means when we need to perform large scale benchmarking (e.g. train 100 models), we could accelerate the process through multiprocessing:"),Object(i.b)("div",{className:"admonition admonition-info alert alert--info"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"})))),"info")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"In ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn"),", ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"../user-guides/distributed#distributed-training"}),"Distributed Training")," doesn't mean training your model on multiple GPUs or multiple machines. Instead, ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"../user-guides/distributed#distributed-training"}),"Distributed Training")," in ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," means training multiple models at the same time."))),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'# With num_jobs=2, we will launch 2 processes to run the tasks in a distributed way.\nresult = cflearn.repeat_with("iris.data", num_repeat=10, num_jobs=2)\n')),Object(i.b)("div",{className:"admonition admonition-caution alert alert--warning"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"16",height:"16",viewBox:"0 0 16 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"})))),"caution")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"On iris dataset, however, launching distributed training will actually hurt the speed because iris dataset only contains 150 samples, so the relative overhead brought by distributed training will be too large. Please refer the ",Object(i.b)("strong",{parentName:"p"},"CAUTION")," section of the ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"../user-guides/distributed#benchmarking"}),"Benchmarking")," section for more details."))),Object(i.b)("h2",{id:"advanced-benchmarking"},"Advanced Benchmarking"),Object(i.b)("p",null,"But this is not enough, because we want to know whether other models (e.g. scikit-learn models) could achieve a better performance than ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," models. In this case, we can perform an advanced benchmarking with the ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"../user-guides/distributed#experiment"}),Object(i.b)("inlineCode",{parentName:"a"},"Experiment"))," helper class."),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'experiment = cflearn.Experiment()\ntr_x, tr_y = m.tr_data.processed.xy\ncv_x, cv_y = m.cv_data.processed.xy\ndata_folder = experiment.dump_data_bundle(tr_x, tr_y, cv_x, cv_y)\n\n# Add carefree-learn tasks\nfor model in ["linear", "fcnn"]:\n    experiment.add_task(model=model, data_folder=data_folder)\n# Add scikit-learn tasks\nrun_command = f"python run_sklearn.py"\ncommon_kwargs = {"run_command": run_command, "data_folder": data_folder}\nexperiment.add_task(model="decision_tree", **common_kwargs)\nexperiment.add_task(model="random_forest", **common_kwargs)\n')),Object(i.b)("p",null,"Notice that we specified ",Object(i.b)("inlineCode",{parentName:"p"},'run_command="python run_sklearn.py"')," for scikit-learn tasks, which means ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"../user-guides/distributed#experiment"}),Object(i.b)("inlineCode",{parentName:"a"},"Experiment"))," will try to execute this command in the current working directory for training scikit-learn models. The good news is that we do not need to speciy any command line arguments, because ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"../user-guides/distributed#experiment"}),Object(i.b)("inlineCode",{parentName:"a"},"Experiment"))," will handle those for us."),Object(i.b)("p",null,"Here is basically what a ",Object(i.b)("inlineCode",{parentName:"p"},"run_sklearn.py")," should look like (",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/carefree0910/carefree-learn/blob/dev/examples/iris/run_sklearn.py"}),"source code"),"):"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'import os\nimport pickle\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom cflearn.dist.runs._utils import get_info\n\nif __name__ == "__main__":\n    info = get_info()\n    kwargs = info.kwargs\n    # data\n    data_list = info.data_list\n    x, y = data_list[:2]\n    # model\n    model = kwargs["model"]\n    if model == "decision_tree":\n        base = DecisionTreeClassifier\n    elif model == "random_forest":\n        base = RandomForestClassifier\n    else:\n        raise NotImplementedError\n    sk_model = base()\n    # train & save\n    sk_model.fit(x, y.ravel())\n    with open(os.path.join(info.workplace, "sk_model.pkl"), "wb") as f:\n        pickle.dump(sk_model, f)\n')),Object(i.b)("p",null,"With ",Object(i.b)("inlineCode",{parentName:"p"},"run_sklearn.py")," defined, we could run those tasks with one line of code:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),"results = experiment.run_tasks()\n")),Object(i.b)("p",null,"After finished running with this, we should be able to see the following file structure in the current working directory:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"|--- __experiment__\n   |--- __data__\n      |-- x.npy\n      |-- y.npy\n      |-- x_cv.npy\n      |-- y_cv.npy\n   |--- fcnn/0\n      |-- _logs\n      |-- __meta__.json\n      |-- cflearn^_^fcnn^_^0000.zip\n   |--- linear/0\n      |-- ...\n   |--- decision_tree/0\n      |-- __meta__.json\n      |-- sk_model.pkl\n   |--- random_forest/0\n      |-- ...\n")),Object(i.b)("p",null,"As we expected, ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," models are saved into zip files, while scikit-learn models are saved into ",Object(i.b)("inlineCode",{parentName:"p"},"sk_model.pkl")," files. Since these models are not yet loaded, we should manually load them into our environment:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'import os\nimport pickle\n\npipelines = {}\nsk_patterns = {}\nfor workplace, workplace_key in zip(results.workplaces, results.workplace_keys):\n        model = workplace_key[0]\n        if model not in ["decision_tree", "random_forest"]:\n            pipelines[model] = cflearn.task_loader(workplace)\n        else:\n            model_file = os.path.join(workplace, "sk_model.pkl")\n            with open(model_file, "rb") as f:\n                sk_model = pickle.load(f)\n                # In `carefree-learn`, we treat labels as column vectors.\n                # So we need to reshape the outputs from the scikit-learn models.\n                sk_predict = lambda x: sk_model.predict(x).reshape([-1, 1])\n                sk_predict_prob = lambda x: sk_model.predict_proba(x)\n                sk_pattern = cflearn.ModelPattern(\n                    predict_method=sk_predict,\n                    predict_prob_method=sk_predict_prob,\n                )\n                sk_patterns[model] = sk_pattern\n')),Object(i.b)("p",null,"After which we can finally perform benchmarking on these models:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),"cflearn.evaluate(cv_x, cv_y, pipelines=pipelines, other_patterns=sk_patterns)\n")),Object(i.b)("p",null,"Which yields"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|     decision_tree      | -- 0.960000 -- | -- 0.000000 -- | -- 0.960000 -- | -- 0.998667 -- | -- 0.000000 -- | -- 0.998667 -- |\n--------------------------------------------------------------------------------------------------------------------------------\n|          fcnn          | -- 0.960000 -- | -- 0.000000 -- | -- 0.960000 -- |    0.994133    | -- 0.000000 -- |    0.994133    |\n--------------------------------------------------------------------------------------------------------------------------------\n|         linear         |    0.466667    | -- 0.000000 -- |    0.466667    |    0.725600    | -- 0.000000 -- |    0.725600    |\n--------------------------------------------------------------------------------------------------------------------------------\n|     random_forest      | -- 0.960000 -- | -- 0.000000 -- | -- 0.960000 -- | -- 0.998667 -- | -- 0.000000 -- | -- 0.998667 -- |\n================================================================================================================================\n")),Object(i.b)("p",null,"Seems that scikit-learn models are better than ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," models! This is not surprising because neural networks often require more data than traditional machine learning algorithms. However, we can boost ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," models with AutoML, as shown in the next section."),Object(i.b)("h2",{id:"automl-on-iris"},"AutoML on\xa0Iris"),Object(i.b)("p",null,"As mentioned in the ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"../"}),Object(i.b)("inlineCode",{parentName:"a"},"Introduction")),", ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," is actually a minimal Automatic Machine Learning (AutoML) solution for tabular datasets. Up till now we haven't mentioned any AutoML stuffs yet, so in this section we'll illustrate how to perform AutoML on Iris dataset, as well as how to pack the AutoML results into production."),Object(i.b)("p",null,"Since ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," has provided the ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"../user-guides/auto-ml"}),Object(i.b)("inlineCode",{parentName:"a"},"cflearn.Auto"))," API for out-of-the-box usages, AutoML in ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," could be achieved in two lines of code:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'auto = cflearn.Auto("clf", models="fcnn")\nauto.fit(tr_x, tr_y, cv_x, cv_y)\n')),Object(i.b)("p",null,"We can make predictions directly with ",Object(i.b)("inlineCode",{parentName:"p"},"auto.predict"),":"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'predictions = auto.predict(cv_x)\nprint("accuracy:", (predictions == cv_y).mean())  # ~0.97\n')),Object(i.b)("p",null,"And of course, we can compare it with other models:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'all_patterns = sk_patterns.copy()\nall_patterns["auto"] = auto.pattern\ncflearn.evaluate(cv_x, cv_y, pipelines=pipelines, other_patterns=all_patterns)\n')),Object(i.b)("p",null,"Which yields"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|          auto          | -- 0.986667 -- | -- 0.000000 -- | -- 0.986667 -- | -- 0.998933 -- | -- 0.000000 -- | -- 0.998933 -- |\n--------------------------------------------------------------------------------------------------------------------------------\n|     decision_tree      |    0.960000    | -- 0.000000 -- |    0.960000    |    0.998667    | -- 0.000000 -- |    0.998667    |\n--------------------------------------------------------------------------------------------------------------------------------\n|          fcnn          |    0.960000    | -- 0.000000 -- |    0.960000    |    0.994133    | -- 0.000000 -- |    0.994133    |\n--------------------------------------------------------------------------------------------------------------------------------\n|         linear         |    0.466667    | -- 0.000000 -- |    0.466667    |    0.725600    | -- 0.000000 -- |    0.725600    |\n--------------------------------------------------------------------------------------------------------------------------------\n|     random_forest      |    0.960000    | -- 0.000000 -- |    0.960000    |    0.998667    | -- 0.000000 -- |    0.998667    |\n================================================================================================================================\n")),Object(i.b)("p",null,"Bravo! Our AutoML model beats the scikit-learn models \ud83e\udd73"),Object(i.b)("p",null,"If we are satisfied with the results, we can pack the models up into a zip file"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'# This will generate a pack.zip file.\nauto.pack("pack")\n')),Object(i.b)("p",null,"which could be used on our production environments / machines easily:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'unpacked = cflearn.Auto.unpack("pack")\npredictions = unpacked.pattern.predict(cv_x)\n')),Object(i.b)("h2",{id:"conclusion"},"Conclusion"),Object(i.b)("p",null,"Contained in this article is just a subset of the features that ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," offers, but we've already walked through many basic & common steps we'll encounter in real life machine learning tasks."))}b.isMDXComponent=!0}}]);