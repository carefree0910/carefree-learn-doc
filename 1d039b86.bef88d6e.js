(window.webpackJsonp=window.webpackJsonp||[]).push([[7],{109:function(e,a,t){"use strict";t.d(a,"a",(function(){return p})),t.d(a,"b",(function(){return u}));var n=t(0),r=t.n(n);function i(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function c(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?c(Object(t),!0).forEach((function(a){i(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):c(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=r.a.createContext({}),d=function(e){var a=r.a.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},p=function(e){var a=d(e.components);return r.a.createElement(l.Provider,{value:a},e.children)},b={inlineCode:"code",wrapper:function(e){var a=e.children;return r.a.createElement(r.a.Fragment,{},a)}},m=r.a.forwardRef((function(e,a){var t=e.components,n=e.mdxType,i=e.originalType,c=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),p=d(t),m=n,u=p["".concat(c,".").concat(m)]||p[m]||b[m]||i;return t?r.a.createElement(u,o(o({ref:a},l),{},{components:t})):r.a.createElement(u,o({ref:a},l))}));function u(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var i=t.length,c=new Array(i);c[0]=m;var o={};for(var s in a)hasOwnProperty.call(a,s)&&(o[s]=a[s]);o.originalType=e,o.mdxType="string"==typeof e?e:n,c[1]=o;for(var l=2;l<i;l++)c[l]=t[l];return r.a.createElement.apply(null,c)}return r.a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},133:function(e,a,t){"use strict";t.r(a),a.default=t.p+"assets/images/pipe-6659ec015a73ac027c5b2a8bdb83360e.png"},75:function(e,a,t){"use strict";t.r(a),t.d(a,"frontMatter",(function(){return c})),t.d(a,"metadata",(function(){return o})),t.d(a,"rightToc",(function(){return s})),t.d(a,"default",(function(){return d}));var n=t(3),r=t(7),i=(t(0),t(109)),c={slug:"intro",title:"Tabular Datasets \u2764\ufe0f\xa0PyTorch",author:"carefree0910",author_title:"Individual Developer",author_url:"https://github.com/carefree0910",author_image_url:"https://avatars2.githubusercontent.com/u/15677328?s=400&u=4f6885246f1b9bb7b22312889859a824c9b05629&v=4",image:"../static/img/title.jpg",tags:["carefree-learn"]},o={permalink:"/carefree-learn-doc/blog/intro",source:"@site/blog/2020-11-23-intro.md",description:"carefree-learn is a minimal Automatic Machine Learning (AutoML) solution for tabular datasets based on PyTorch. It is the 2nd-place winner in the Global PyTorch Summer Hackathon 2020. The library is documented and you can try it out after installing it.",date:"2020-11-23T00:00:00.000Z",tags:[{label:"carefree-learn",permalink:"/carefree-learn-doc/blog/tags/carefree-learn"}],title:"Tabular Datasets \u2764\ufe0f\xa0PyTorch",readingTime:13.795,truncated:!0},s=[{value:"Why carefree-learn?",id:"why-carefree-learn",children:[{value:"Carefree using",id:"carefree-using",children:[]},{value:"Carefree developing",id:"carefree-developing",children:[]}]},{value:"Why PyTorch?",id:"why-pytorch",children:[]},{value:"Getting Started with Iris",id:"getting-started-with-iris",children:[{value:"Inspect the Iris Dataset",id:"inspect-the-iris-dataset",children:[]},{value:"Basic Usages",id:"basic-usages",children:[]},{value:"Benchmarking",id:"benchmarking",children:[]},{value:"Advanced Benchmarking",id:"advanced-benchmarking",children:[]},{value:"AutoML on Iris",id:"automl-on-iris",children:[]}]},{value:"Conclusions",id:"conclusions",children:[]}],l={rightToc:s};function d(e){var a=e.components,c=Object(r.a)(e,["components"]);return Object(i.b)("wrapper",Object(n.a)({},l,c,{components:a,mdxType:"MDXLayout"}),Object(i.b)("p",null,Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," is a minimal Automatic Machine Learning (AutoML) solution for tabular datasets based on PyTorch. It is the 2nd-place winner in the Global PyTorch Summer Hackathon 2020. The library is ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs"}),"documented")," and you can try it out after ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/getting-started/installation"}),"installing it"),"."),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},Object(i.b)("a",Object(n.a)({parentName:"li"},{href:"https://github.com/carefree0910/carefree-learn"}),"GitHub")),Object(i.b)("li",{parentName:"ul"},Object(i.b)("a",Object(n.a)({parentName:"li"},{href:"https://www.youtube.com/watch?v=hMzLmwmdQ_k&t=10s"}),"YouTube"))),Object(i.b)("h2",{id:"why-carefree-learn"},"Why carefree-learn?"),Object(i.b)("p",null,"If you are familiar with machine learning, you may already heard about scikit-learn and some other automl frameworks. The motivation behind creating carefree-learn was two fold:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"Leverage ",Object(i.b)("a",Object(n.a)({parentName:"li"},{href:"https://pytorch.org/"}),"PyTorch")," in the field of machine learning."),Object(i.b)("li",{parentName:"ul"},"Provide a truly CAREFREE experience for both users and developers.")),Object(i.b)("p",null,"By saying carefree we mean that both using and developing carefree-learn could be finished in one line of code:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'import cflearn\n\n# train a neural network on x & y\nm = cflearn.make().fit(x, y)\n\n# defining your new model and train with it\ncflearn.register_model("awesome_model", pipes=[cflearn.PipeInfo("dndf")])\nm = cflearn.make("awesome_model").fit(x, y)\n')),Object(i.b)("div",{className:"admonition admonition-note alert alert--secondary"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})))),"note")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"Please refer to ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/getting-started/quick-start"}),"Quick Start")," and ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/developer-guides/customization"}),"Build Your Own Models")," for more details."))),Object(i.b)("p",null,"You may argue that other libraries, like scikit-learn, also support building models in one line of code. But think about the messy stuffs (such as reading data from files, exploring data, performing data pre-processing, etc.) we need to do before we actually use these libraries, and contributing algorithms to these libraries is often much harder than writing your own ones. In carefree-learn, we've tried hard to ease these two procedures for you."),Object(i.b)("h3",{id:"carefree-using"},"Carefree using"),Object(i.b)("p",null,"As mentioned, other libraries (e.g. scikit-learn) can often only support ",Object(i.b)("inlineCode",{parentName:"p"},"numpy")," arrays or ",Object(i.b)("inlineCode",{parentName:"p"},"DataFrame")," as input, and have pretty much constraints (e.g. cannot contain ",Object(i.b)("inlineCode",{parentName:"p"},"nan")," values). In carefree-learn, however, we've tried hard to help you deal with almost ",Object(i.b)("strong",{parentName:"p"},"ANY")," kind of tabular datasets, no matter how dirty and messy it is. This means carefree-learn actually provides an end-to-end pipeline on tabular datasets, including ",Object(i.b)("strong",{parentName:"p"},"AUTOMATICALLY")," deal with:"),Object(i.b)("ul",null,Object(i.b)("li",{parentName:"ul"},"Detection of redundant feature columns which can be excluded (all SAME, all DIFFERENT, etc)."),Object(i.b)("li",{parentName:"ul"},"Detection of feature columns types (whether a feature column is string column / numerical column / categorical column)."),Object(i.b)("li",{parentName:"ul"},"Encoding of string columns and categorical columns (Embedding or One Hot Encoding)."),Object(i.b)("li",{parentName:"ul"},"Pre-processing of numerical columns (Normalize, Min Max, etc.)."),Object(i.b)("li",{parentName:"ul"},"Imputation of missing values."),Object(i.b)("li",{parentName:"ul"},"And much more\u2026")),Object(i.b)("p",null,"Therefore, carefree-learn is possible to procss files directly (file-in, file-out):"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'import cflearn\n\n# train a neural network on train.csv\nm = cflearn.make().fit("train.csv")\n# make predictions on test.csv\npredictions = m.predict("test.csv")\n')),Object(i.b)("div",{className:"admonition admonition-note alert alert--secondary"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})))),"note")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"This is mainly handled by ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/carefree0910/carefree-data"}),"carefree-data"),", part of the ",Object(i.b)("inlineCode",{parentName:"p"},"cf*")," ecosystem"))),Object(i.b)("div",{className:"admonition admonition-tip alert alert--success"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"})))),"tip")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"Please refer to ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/getting-started/quick-start"}),"Quick Start")," for more details."))),Object(i.b)("h3",{id:"carefree-developing"},"Carefree developing"),Object(i.b)("p",null,"Thanks to the great modularization provided by ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://pytorch.org/"}),"PyTorch"),", ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," was able to design its (tabular datasets oriented) pipeline in a user-friendly as well as a developer-friendly way."),Object(i.b)("p",null,"The basic component in ",Object(i.b)("inlineCode",{parentName:"p"},"carefree-learn")," is called a ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/design-principles#pipe"}),Object(i.b)("inlineCode",{parentName:"a"},"pipe")),", which corresponds to one of those ",Object(i.b)("em",{parentName:"p"},"branches")," which takes in all / part of the inputs, apply some ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/design-principles#transform"}),Object(i.b)("inlineCode",{parentName:"a"},"transform")),", extract some features with ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/design-principles#extractor"}),Object(i.b)("inlineCode",{parentName:"a"},"extractor")),", and then feed the final network (",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/design-principles#head"}),Object(i.b)("inlineCode",{parentName:"a"},"head")),") with these features. Here's an example:"),Object(i.b)("p",null,Object(i.b)("img",{alt:"Pipe",src:t(133).default})),Object(i.b)("p",null,"Since most of the deep learning models (neural networks) used in tabular datasets could be represented with ",Object(i.b)("inlineCode",{parentName:"p"},"pipe"),", developers can therefore focus on implementing one of its components (namely ",Object(i.b)("inlineCode",{parentName:"p"},"extractor")," and ",Object(i.b)("inlineCode",{parentName:"p"},"head"),"), instead of having to care about the whole pipeline."),Object(i.b)("p",null,"We've provided a detailed documention on ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/developer-guides/customization"}),"how to build your own models"),", as well as an interesting ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/examples/operations"}),"example")," to guide you step by step. Please feel free to check them out and enjoy yourself!"),Object(i.b)("h2",{id:"why-pytorch"},"Why PyTorch?"),Object(i.b)("p",null,"The reason why I choosed ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://pytorch.org/"}),"PyTorch")," has already been mentioned above\u200a-\u200ait is perfectly modularized and customizable. Another reason is that although deep learning outshines in CV and NLP fields, it is not as popular when it comes to the tabular datasets. We think the main reason is that tabular datasets require so many messy steps before we could actually jump into the algorithm part, and there has not existed a modularized framework to organize these stuffs. That's why we tried our best to handle all of these for you and hope that it could help you focus on developing the core algorithms. And among the deep learning frameworks, ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://pytorch.org/"}),"PyTorch")," was truly the most elegant one and we were deeply attracted by its simplicity and power. Since there lacks a satisfying 'carefree' solution for tabular datasets,  we decided to take advantage of our knowledges and build one ourselves. So here comes the carefree-learn, which aims to provide out of the box tools to train and develop neural networks on tabular datasets with ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://pytorch.org/"}),"PyTorch"),"."),Object(i.b)("h2",{id:"getting-started-with-iris"},"Getting Started with Iris"),Object(i.b)("p",null,"We've provided an ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/getting-started/installation"}),"Installation Guide")," as well as some real life ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/examples/iris"}),"Examples")," to walk you through the basic / advanced usages of carefree-learn. We've also provided a ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/user-guides/production"}),"Production Guide")," to show how could we pack the whole pipeline in carefree-learn efficiently into a zip file. In this section, we\u2019ll use the famous ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://archive.ics.uci.edu/ml/datasets/iris"}),"Iris dataset")," to demonstrate the power of carefree-learn."),Object(i.b)("div",{className:"admonition admonition-info alert alert--info"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"})))),"info")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"We\u2019ve provided the complete source code ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/carefree0910/carefree-learn/blob/dev/examples/iris/iris.py"}),"here")," as well as a jupyter notebook ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/carefree0910/carefree-learn/blob/dev/examples/iris/iris.ipynb"}),"here"),"."))),Object(i.b)("h3",{id:"inspect-the-iris-dataset"},"Inspect the Iris Dataset"),Object(i.b)("p",null,"Here are some of the information provided by the official website:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"This is perhaps the best known database to be found in the pattern recognition literature.\nThe data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant.\nPredicted attribute: class of iris plant.\n")),Object(i.b)("p",null,"And here\u2019s the pandas-view of the raw data (we didn\u2019t use pandas in our code, but it is convenient to visualize some data with it though \ud83e\udd23):"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"      f0   f1   f2   f3           label\n0    5.1  3.5  1.4  0.2     Iris-setosa\n1    4.9  3.0  1.4  0.2     Iris-setosa\n2    4.7  3.2  1.3  0.2     Iris-setosa\n3    4.6  3.1  1.5  0.2     Iris-setosa\n4    5.0  3.6  1.4  0.2     Iris-setosa\n..   ...  ...  ...  ...             ...\n145  6.7  3.0  5.2  2.3  Iris-virginica\n146  6.3  2.5  5.0  1.9  Iris-virginica\n147  6.5  3.0  5.2  2.0  Iris-virginica\n148  6.2  3.4  5.4  2.3  Iris-virginica\n149  5.9  3.0  5.1  1.8  Iris-virginica\n\n[150 rows x 5 columns]\n")),Object(i.b)("div",{className:"admonition admonition-note alert alert--secondary"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})))),"note")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"You can download the raw data (",Object(i.b)("strong",{parentName:"p"},"iris.data"),") with ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"}),"this link"),"."))),Object(i.b)("h3",{id:"basic-usages"},"Basic Usages"),Object(i.b)("p",null,"Traditionally, we need to process the raw data before we feed them into our machine learning models (e.g. encode the label column, which is a string column, into an ordinal column). In carefree-learn, however, we can train neural networks directly on files without worrying about the rest:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'import cflearn\n\nm = cflearn.make().fit("iris.data")\n')),Object(i.b)("p",null,"What\u2019s going under the hood is that carefree-learn will try to parse the ",Object(i.b)("inlineCode",{parentName:"p"},"iris.data")," automatically (with the help of ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/carefree0910/carefree-data"}),"carefree-data"),"), split the data into training set and validation set, with which we\u2019ll train a fully connected neural network (",Object(i.b)("inlineCode",{parentName:"p"},"fcnn"),")."),Object(i.b)("p",null,"We can further inspect the processed data if we want to know how carefree-learn actually parsed the input data:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),"# print out the first elements in the dataset\nprint(m.tr_data.raw.x[0])\nprint(m.tr_data.raw.y[0])\nprint(m.tr_data.processed.x[0])\nprint(m.tr_data.processed.y[0])\n\n\"\"\"\n['4.6', '3.6', '1.0', '0.2']\n['Iris-setosa']\n[-1.5065205  1.2634597 -1.5687355 -1.3129768]\n[0]\n\"\"\"\n")),Object(i.b)("p",null,"It shows that the raw data is carefully normalized into numerical data that neural networks can accept. You may also notice that the first elements are not identical with the first line of the raw data, this is caused by the auto-shuffle mechanism introduced in ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://github.com/carefree0910/carefree-data"}),"carefree-data"),")."),Object(i.b)("p",null,"What\u2019s more, by saying ",Object(i.b)("em",{parentName:"p"},"normalized"),", it means that the input features will be automatically normalized to ",Object(i.b)("inlineCode",{parentName:"p"},"mean=0.0")," and ",Object(i.b)("inlineCode",{parentName:"p"},"std=1.0"),":"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'import numpy as np\n\ntr_x = m.tr_data.processed.x\ncv_x = m.cv_data.processed.x\nstacked = np.vstack([tr_x, cv_x])\nprint(stacked.mean(0))\nprint(stacked.std(0))\n\n"""\n[ 3.1739475e-08 -3.7471455e-07 -1.9907951e-07 -8.0267590e-08]\n[0.99999976 0.9999997  1.0000002  0.9999999 ]\n"""\n')),Object(i.b)("div",{className:"admonition admonition-info alert alert--info"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"})))),"info")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"This means we first normalized the data before we actually split it into train & validation set."))),Object(i.b)("p",null,"After training on files, carefree-learn can predict & evaluate on files directly as well. We\u2019ll handle the data parsing and normalization for you automatically:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'# `contains_labels` is set to True because `iris.data` itself contains labels\npredictions = m.predict("iris.data", contains_labels=True)\n# evaluations could be achieved easily with cflearn.evaluate\ncflearn.evaluate("iris.data", pipelines=m)\n')),Object(i.b)("p",null,"Which yields"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|          fcnn          |    0.926667    |    0.000000    |    0.926667    |    0.994800    |    0.000000    |    0.994800    |\n================================================================================================================================\n")),Object(i.b)("h3",{id:"benchmarking"},"Benchmarking"),Object(i.b)("p",null,"As we know, neural networks are trained with stochastic gradient descent (and its variants), which will introduce some randomness to the final result, even if we are training on the same dataset. In this case, we need to repeat the same task several times in order to obtain the bias & variance of our neural networks. Fortunately, carefree-learn introduced ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/user-guides/distributed#repeat_with"}),Object(i.b)("inlineCode",{parentName:"a"},"repeat_with"))," API, which can achieve this goal easily with only a few lines of code:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'# With num_repeat=3 specified, we\u2019ll train 3 models on iris.data\nresult = cflearn.repeat_with("iris.data", num_repeat=3)\ncflearn.evaluate("iris.data", pipelines=result.pipelines)\n')),Object(i.b)("p",null,"Which yields"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|          fcnn          |    0.902222    |    0.019116    |    0.883106    |    0.985778    |    0.004722    |    0.981055    |\n================================================================================================================================\n")),Object(i.b)("p",null,"We can also compare the performances across different models:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'# With models=["linear", "fcnn"], we\'ll train both linear models and fcnn models.\nmodels = ["linear", "fcnn"]\nresult = cflearn.repeat_with("iris.data", models=models, num_repeat=3)\ncflearn.evaluate("iris.data", pipelines=result.pipelines)\n')),Object(i.b)("p",null,"Which yields"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|          fcnn          | -- 0.915556 -- | -- 0.027933 -- | -- 0.887623 -- | -- 0.985467 -- | -- 0.004121 -- | -- 0.981345 -- |\n--------------------------------------------------------------------------------------------------------------------------------\n|         linear         |    0.620000    |    0.176970    |    0.443030    |    0.733778    |    0.148427    |    0.585351    |\n================================================================================================================================\n")),Object(i.b)("p",null,"It is worth mentioning that carefree-learn supports ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/user-guides/distributed/#distributed-training"}),"distributed training"),", which means when we need to perform large scale benchmarking (e.g. train 100 models), we could accelerate the process through multiprocessing:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'# With num_jobs=2, we will launch 2 processes to run the tasks in a distributed way.\nresult = cflearn.repeat_with("iris.data", num_repeat=10, num_jobs=2)\n')),Object(i.b)("div",{className:"admonition admonition-note alert alert--secondary"},Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(i.b)("h5",{parentName:"div"},Object(i.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(i.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(i.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})))),"note")),Object(i.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(i.b)("p",{parentName:"div"},"In carefree-learn, distributed training doesn't mean training your model on multiple GPUs or multiple machines. Instead, distributed training in carefree-learn means training multiple models at the same time. Please refer to our ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/user-guides/distributed/#distributed-training"}),"documentation")," for more details."))),Object(i.b)("p",null,"On iris dataset, however, launching distributed training will actually hurt the speed because iris dataset only contains 150 samples, so the relative overhead brought by distributed training will be too large. Please refer the ",Object(i.b)("strong",{parentName:"p"},"CAUTION")," section of our ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/user-guides/distributed/#benchmarking"}),"documentation")," for more details."),Object(i.b)("h3",{id:"advanced-benchmarking"},"Advanced Benchmarking"),Object(i.b)("p",null,"But this is not enough, because we want to know whether other models (e.g. scikit-learn models) could achieve a better performance than carefree-learn models. In this case, we can perform an advanced benchmarking with the ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/user-guides/distributed/#experiment"}),"Experiment")," helper class."),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'experiment = cflearn.Experiment()\ntr_x, tr_y = m.tr_data.processed.xy\ncv_x, cv_y = m.cv_data.processed.xy\ndata_folder = experiment.dump_data_bundle(tr_x, tr_y, cv_x, cv_y)\n\n# Add carefree-learn tasks\nfor model in ["linear", "fcnn"]:\n    experiment.add_task(model=model, data_folder=data_folder)\n# Add scikit-learn tasks\nrun_command = f"python run_sklearn.py"\ncommon_kwargs = {"run_command": run_command, "data_folder": data_folder}\nexperiment.add_task(model="decision_tree", **common_kwargs)\nexperiment.add_task(model="random_forest", **common_kwargs)\n')),Object(i.b)("p",null,'Notice that we specified run_command="python run_sklearn.py" for scikit-learn tasks, which means ',Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/user-guides/distributed/#experiment"}),"Experiment")," will try to execute this command in the current working directory for training scikit-learn models. The good news is that we do not need to speciy any command line arguments, because ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/user-guides/distributed/#experiment"}),"Experiment")," will handle those for us."),Object(i.b)("p",null,"Here is basically what a ",Object(i.b)("inlineCode",{parentName:"p"},"run_sklearn.py")," should look like (",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"http://localhost:8888/files/run_sklearn.py"}),"source code"),"):"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'import os\nimport pickle\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom cflearn.dist.runs._utils import get_info\n\nif __name__ == "__main__":\n    info = get_info()\n    kwargs = info.kwargs\n    # data\n    data_list = info.data_list\n    x, y = data_list[:2]\n    # model\n    model = kwargs["model"]\n    if model == "decision_tree":\n        base = DecisionTreeClassifier\n    elif model == "random_forest":\n        base = RandomForestClassifier\n    else:\n        raise NotImplementedError\n    sk_model = base()\n    # train & save\n    sk_model.fit(x, y.ravel())\n    with open(os.path.join(info.workplace, "sk_model.pkl"), "wb") as f:\n        pickle.dump(sk_model, f)\n')),Object(i.b)("p",null,"With ",Object(i.b)("inlineCode",{parentName:"p"},"run_sklearn.py")," defined, we could run those tasks with one line of code:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),"results = experiment.run_tasks()\n")),Object(i.b)("p",null,"After finished running, we should be able to see the following file structure in the current working directory:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"|--- __experiment__\n   |--- __data__\n      |-- x.npy\n      |-- y.npy\n      |-- x_cv.npy\n      |-- y_cv.npy\n   |--- fcnn/0\n      |-- _logs\n      |-- __meta__.json\n      |-- cflearn^_^fcnn^_^0000.zip\n   |--- linear/0\n      |-- ...\n   |--- decision_tree/0\n      |-- __meta__.json\n      |-- sk_model.pkl\n   |--- random_forest/0\n      |-- ...\n")),Object(i.b)("p",null,"As we expected, carefree-learn models are saved into zip files, while scikit-learn models are saved into ",Object(i.b)("inlineCode",{parentName:"p"},"sk_model.pkl")," files. Since these models are not yet loaded, we should manually load them into our environment:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'pipelines = {}\nsk_patterns = {}\nfor workplace, workplace_key in zip(results.workplaces, results.workplace_keys):\n    model = workplace_key[0]\n    if model not in ["decision_tree", "random_forest"]:\n        pipelines[model] = cflearn.task_loader(workplace)\n    else:\n        model_file = os.path.join(workplace, "sk_model.pkl")\n        with open(model_file, "rb") as f:\n            sk_model = pickle.load(f)\n            # In `carefree-learn`, we treat labels as column vectors.\n            # So we need to reshape the outputs from the scikit-learn models.\n            sk_predict = lambda x: sk_model.predict(x).reshape([-1, 1])\n            sk_predict_prob = lambda x: sk_model.predict_proba(x)\n            sk_pattern = cflearn.ModelPattern(\n                predict_method=sk_predict,\n                predict_prob_method=sk_predict_prob,\n            )\n            sk_patterns[model] = sk_pattern\n')),Object(i.b)("p",null,"After which we can finally perform benchmarking on these models:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),"cflearn.evaluate(cv_x, cv_y, pipelines=pipelines, other_patterns=sk_patterns)\n")),Object(i.b)("p",null,"Which yields"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"~~~  [ info ] Results\n================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|     decision_tree      | -- 0.960000 -- | -- 0.000000 -- | -- 0.960000 -- | -- 0.998667 -- | -- 0.000000 -- | -- 0.998667 -- |\n--------------------------------------------------------------------------------------------------------------------------------\n|          fcnn          | -- 0.960000 -- | -- 0.000000 -- | -- 0.960000 -- |    0.994133    | -- 0.000000 -- |    0.994133    |\n--------------------------------------------------------------------------------------------------------------------------------\n|         linear         |    0.466667    | -- 0.000000 -- |    0.466667    |    0.725600    | -- 0.000000 -- |    0.725600    |\n--------------------------------------------------------------------------------------------------------------------------------\n|     random_forest      | -- 0.960000 -- | -- 0.000000 -- | -- 0.960000 -- | -- 0.998667 -- | -- 0.000000 -- | -- 0.998667 -- |\n================================================================================================================================\n")),Object(i.b)("p",null,"Seems that scikit-learn models are better than carefree-learn models! This is not surprising because neural networks often require more data than traditional machine learning algorithms. However, we can boost carefree-learn models with AutoML, as shown in the next section."),Object(i.b)("h3",{id:"automl-on-iris"},"AutoML on Iris"),Object(i.b)("p",null,"As mentioned in the introduction, carefree-learn is actually a minimal Automatic Machine Learning (AutoML) solution for tabular datasets. Up till now we haven't mentioned any AutoML stuffs yet, so in this section we'll illustrate how to perform AutoML on Iris dataset, as well as how to pack the AutoML results into production."),Object(i.b)("p",null,"Since carefree-learn has provided the ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/user-guides/auto-ml/"}),"cflearn.Auto")," API for out-of-the-box usages, AutoML in carefree-learn could be achieved in two lines of code:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'auto = cflearn.Auto("clf", models="fcnn")\nauto.fit(tr_x, tr_y, cv_x, cv_y)\n')),Object(i.b)("p",null,"We can make predictions directly with ",Object(i.b)("inlineCode",{parentName:"p"},"auto.predict"),":"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'predictions = auto.predict(cv_x)\nprint("accuracy:", (predictions == cv_y).mean())  # ~0.985\n')),Object(i.b)("p",null,"And of course, we can compare it with other models:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'all_patterns = sk_patterns.copy()\nall_patterns["auto"] = auto.pattern\ncflearn.evaluate(cv_x, cv_y, pipelines=pipelines, other_patterns=all_patterns)\n')),Object(i.b)("p",null,"Which yields"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-text"}),"================================================================================================================================\n|        metrics         |                       acc                        |                       auc                        |\n--------------------------------------------------------------------------------------------------------------------------------\n|                        |      mean      |      std       |     score      |      mean      |      std       |     score      |\n--------------------------------------------------------------------------------------------------------------------------------\n|          auto          | -- 0.986667 -- | -- 0.000000 -- | -- 0.986667 -- | -- 0.998933 -- | -- 0.000000 -- | -- 0.998933 -- |\n--------------------------------------------------------------------------------------------------------------------------------\n|     decision_tree      |    0.960000    | -- 0.000000 -- |    0.960000    |    0.998667    | -- 0.000000 -- |    0.998667    |\n--------------------------------------------------------------------------------------------------------------------------------\n|          fcnn          |    0.960000    | -- 0.000000 -- |    0.960000    |    0.994133    | -- 0.000000 -- |    0.994133    |\n--------------------------------------------------------------------------------------------------------------------------------\n|         linear         |    0.466667    | -- 0.000000 -- |    0.466667    |    0.725600    | -- 0.000000 -- |    0.725600    |\n--------------------------------------------------------------------------------------------------------------------------------\n|     random_forest      |    0.960000    | -- 0.000000 -- |    0.960000    |    0.998667    | -- 0.000000 -- |    0.998667    |\n================================================================================================================================\n")),Object(i.b)("p",null,"Bravo! Our AutoML model beats the scikit-learn models \ud83e\udd73"),Object(i.b)("p",null,"If we are satisfied with the results, we can pack the models up into a zip file"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'auto.pack("pack")\n')),Object(i.b)("p",null,"which could be used on our production environments / machines easily:"),Object(i.b)("pre",null,Object(i.b)("code",Object(n.a)({parentName:"pre"},{className:"language-python"}),'unpacked = cflearn.Auto.unpack("pack")\npredictions = unpacked.pattern.predict(cv_x)\n')),Object(i.b)("h2",{id:"conclusions"},"Conclusions"),Object(i.b)("p",null,"Contained in this article is just a subset of the features that carefree-learn offers, but we\u2019ve already walked through many basic & common steps we\u2019ll encounter in real life machine learning tasks. Additional capabilities include ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/optimizations#embedding"}),"Fast Embedding"),", ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/optimizations#one-hot-encoding"}),"One Hot Encoding Caching"),", ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/developer-guides/customization"}),"highly customizable modules"),", specify / customize optimizers / lr schedulers, ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/user-guides/production"}),"export to ONNX"),", and more."),Object(i.b)("p",null,"To learn more about carefree-learn, check out the ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/examples/Titanic"}),"Examples")," with jupyter notebooks included, as well as the ",Object(i.b)("a",Object(n.a)({parentName:"p"},{href:"https://carefree0910.me/carefree-learn-doc/docs/developer-guides/customization"}),"Developer\u2019s Guide")," to see how do we customize models with carefree-learn. We sincerely hope that carefree-learn could help you either deal with tabular datasets easier or develop new algorithms on tabular datasets easier, and any contributions to carefree-learn would truly be welcomed."))}d.isMDXComponent=!0}}]);