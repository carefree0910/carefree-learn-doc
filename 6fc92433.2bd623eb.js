(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{83:function(e,n,t){"use strict";t.r(n),t.d(n,"frontMatter",(function(){return i})),t.d(n,"metadata",(function(){return l})),t.d(n,"rightToc",(function(){return s})),t.d(n,"default",(function(){return p}));var a=t(3),r=t(7),o=(t(0),t(97)),i={id:"MNIST",title:"MNIST"},l={unversionedId:"examples/MNIST",id:"examples/MNIST",isDocsHomePage:!1,title:"MNIST",description:"The MNIST dataset is a dataset of handwritten digits that is commonly used as the 'Hello World' dataset in Deep Learning domain. It contains 60,000 training images and 10,000 testing images, and",source:"@site/docs/examples/mnist.md",slug:"/examples/MNIST",permalink:"/carefree-learn-doc/docs/examples/MNIST",version:"current",lastUpdatedAt:1635384026,sidebar:"docs",previous:{title:"Titanic",permalink:"/carefree-learn-doc/docs/examples/Titanic"},next:{title:"General",permalink:"/carefree-learn-doc/docs/user-guides/general"}},s=[{value:"Classification",id:"classification",children:[]},{value:"Variational Auto Encoder",id:"variational-auto-encoder",children:[]},{value:"Generative Adversarial Network",id:"generative-adversarial-network",children:[]}],c={rightToc:s};function p(e){var n=e.components,t=Object(r.a)(e,["components"]);return Object(o.b)("wrapper",Object(a.a)({},c,t,{components:n,mdxType:"MDXLayout"}),Object(o.b)("p",null,"The MNIST dataset is a dataset of handwritten digits that is commonly used as the 'Hello World' dataset in Deep Learning domain. It contains 60,000 training images and 10,000 testing images, and\n",Object(o.b)("inlineCode",{parentName:"p"},"carefree-learn")," provided a straightforward API to access it."),Object(o.b)("p",null,"MNIST dataset can be used for training various image processing systems. In this article, we will demonstrate how to actually utilize ",Object(o.b)("inlineCode",{parentName:"p"},"carefree-learn")," to solve these different tasks on MNIST dataset."),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),'# preparations\n\nimport torch\nimport cflearn\n\nimport numpy as np\nimport torch.nn as nn\n\n# MNIST dataset could be prepared with this one line of code\ndata = cflearn.cv.MNISTData(batch_size=16, transform="to_tensor")\n\n# for reproduction\nnp.random.seed(142857)\ntorch.manual_seed(142857)\n')),Object(o.b)("div",{className:"admonition admonition-tip alert alert--success"},Object(o.b)("div",Object(a.a)({parentName:"div"},{className:"admonition-heading"}),Object(o.b)("h5",{parentName:"div"},Object(o.b)("span",Object(a.a)({parentName:"h5"},{className:"admonition-icon"}),Object(o.b)("svg",Object(a.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"}),Object(o.b)("path",Object(a.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"})))),"tip")),Object(o.b)("div",Object(a.a)({parentName:"div"},{className:"admonition-content"}),Object(o.b)("ul",{parentName:"div"},Object(o.b)("li",{parentName:"ul"},"As shown above, the MNIST dataset could be easily turned into a ",Object(o.b)("inlineCode",{parentName:"li"},"DLDataModule")," instance, which is the common data interface used in ",Object(o.b)("inlineCode",{parentName:"li"},"carefree-learn"),"."),Object(o.b)("li",{parentName:"ul"},"The ",Object(o.b)("inlineCode",{parentName:"li"},"transform")," argument specifies which transform do we want to use to pre-process the input batch. See ",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"/docs/user-guides/computer-vision#transforms"}),"Transforms")," section for more details.")))),Object(o.b)("h2",{id:"classification"},"Classification"),Object(o.b)("table",null,Object(o.b)("thead",{parentName:"table"},Object(o.b)("tr",{parentName:"thead"},Object(o.b)("th",Object(a.a)({parentName:"tr"},{align:"center"}),"Python source code"),Object(o.b)("th",Object(a.a)({parentName:"tr"},{align:"center"}),"Jupyter Notebook"),Object(o.b)("th",Object(a.a)({parentName:"tr"},{align:"center"}),"Task"))),Object(o.b)("tbody",{parentName:"table"},Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",Object(a.a)({parentName:"tr"},{align:"center"}),Object(o.b)("a",Object(a.a)({parentName:"td"},{href:"https://github.com/carefree0910/carefree-learn/blob/dev/examples/cv/mnist/run_clf.py"}),"run_clf.py")),Object(o.b)("td",Object(a.a)({parentName:"tr"},{align:"center"}),Object(o.b)("a",Object(a.a)({parentName:"td"},{href:"https://nbviewer.jupyter.org/github/carefree0910/carefree-learn/blob/dev/examples/cv/mnist/clf.ipynb"}),"clf.ipynb")),Object(o.b)("td",Object(a.a)({parentName:"tr"},{align:"center"}),"Computer Vision \ud83d\uddbc\ufe0f")))),Object(o.b)("p",null,"For demo purpose, we are going to build a simple convolution-based classifier:"),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),'@cflearn.register_module("simple_conv")\nclass SimpleConvClassifier(nn.Sequential):\n    def __init__(self, in_channels: int, num_classes: int):\n        super().__init__(\n            nn.Conv2d(in_channels, 16, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(16),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(1),\n            nn.Linear(128, num_classes),\n        )\n')),Object(o.b)("p",null,"We leveraged the ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"/docs/developer-guides/computer-vision-customization#customize-models"}),Object(o.b)("inlineCode",{parentName:"a"},"register_module"))," API here, which can turn a general ",Object(o.b)("inlineCode",{parentName:"p"},"nn.Module")," instance to a ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"/docs/design-principles/#model"}),Object(o.b)("inlineCode",{parentName:"a"},"ModelProtocol"))," in ",Object(o.b)("inlineCode",{parentName:"p"},"carefree-learn"),". After registered, it can be easily accessed with its name (",Object(o.b)("inlineCode",{parentName:"p"},'"simple_conv"'),"):"),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),'cflearn.api.fit_cv(\n    data,\n    "simple_conv",\n    {"in_channels": 1, "num_classes": 10},\n    loss_name="cross_entropy",\n    metric_names="acc",\n    fixed_epoch=1,                                  # for demo purpose, we only train our model for 1 epoch\n    cuda=0 if torch.cuda.is_available() else None,  # use CUDA if possible\n)\n')),Object(o.b)("p",null,"Our model achieves 98.0400% accuracy on validation set within 1 epoch, not bad!"),Object(o.b)("h2",{id:"variational-auto-encoder"},"Variational Auto Encoder"),Object(o.b)("table",null,Object(o.b)("thead",{parentName:"table"},Object(o.b)("tr",{parentName:"thead"},Object(o.b)("th",Object(a.a)({parentName:"tr"},{align:"center"}),"Python source code"),Object(o.b)("th",Object(a.a)({parentName:"tr"},{align:"center"}),"Jupyter Notebook"),Object(o.b)("th",Object(a.a)({parentName:"tr"},{align:"center"}),"Task"))),Object(o.b)("tbody",{parentName:"table"},Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",Object(a.a)({parentName:"tr"},{align:"center"}),Object(o.b)("a",Object(a.a)({parentName:"td"},{href:"https://github.com/carefree0910/carefree-learn/blob/dev/examples/cv/mnist/run_vae.py"}),"run_vae.py")),Object(o.b)("td",Object(a.a)({parentName:"tr"},{align:"center"}),Object(o.b)("a",Object(a.a)({parentName:"td"},{href:"https://nbviewer.jupyter.org/github/carefree0910/carefree-learn/blob/dev/examples/cv/mnist/vae.ipynb"}),"vae.ipynb")),Object(o.b)("td",Object(a.a)({parentName:"tr"},{align:"center"}),"Computer Vision \ud83d\uddbc\ufe0f")))),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),"import torch.nn.functional as F\n\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Optional\nfrom cflearn.types import losses_type\nfrom cflearn.types import tensor_dict_type\nfrom cflearn.protocol import TrainerState\nfrom cflearn.misc.toolkit import interpolate\nfrom cflearn.modules.blocks import Lambda\nfrom cflearn.modules.blocks import UpsampleConv2d\n")),Object(o.b)("p",null,"For demo purpose, we are going to build a simple convolution-based VAE:"),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),'@cflearn.register_module("simple_vae")\nclass SimpleVAE(nn.Module):\n    def __init__(self, in_channels: int, img_size: int):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(in_channels, 16, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(16),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(1),\n        )\n        self.decoder = nn.Sequential(\n            Lambda(lambda t: t.view(-1, 4, 4, 4), name="reshape"),\n            nn.Conv2d(4, 128, 1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(128),\n            UpsampleConv2d(128, 64, kernel_size=3, padding=1, factor=2),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            UpsampleConv2d(64, 32, kernel_size=3, padding=1, factor=2),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(32),\n            UpsampleConv2d(32, in_channels, kernel_size=3, padding=1, factor=2),\n            Lambda(lambda t: interpolate(t, size=img_size, mode="bilinear")),\n        )\n\n    def forward(self, net: torch.Tensor) -> Dict[str, torch.Tensor]:\n        net = self.encoder(net)\n        mu, log_var = net.chunk(2, dim=1)\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        net = eps * std + mu\n        net = self.decoder(net)\n        return {"mu": mu, "log_var": log_var, cflearn.PREDICTIONS_KEY: net}\n')),Object(o.b)("p",null,"There are quite a few details that worth to be mentioned:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"We leveraged the ",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"/docs/developer-guides/computer-vision-customization#customize-models"}),Object(o.b)("inlineCode",{parentName:"a"},"register_module"))," API here, which can turn a general ",Object(o.b)("inlineCode",{parentName:"li"},"nn.Module")," instance to a ",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"/docs/design-principles#model"}),Object(o.b)("inlineCode",{parentName:"a"},"ModelProtocol"))," in ",Object(o.b)("inlineCode",{parentName:"li"},"carefree-learn"),". After registered, it can be easily accessed with its name (",Object(o.b)("inlineCode",{parentName:"li"},'"simple_vae"'),")"),Object(o.b)("li",{parentName:"ul"},"We leveraged some built-in ",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"/docs/design-principles#common-blocks"}),"common blocks")," of ",Object(o.b)("inlineCode",{parentName:"li"},"carefree-learn")," to build our simple VAE:",Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},Object(o.b)("inlineCode",{parentName:"li"},"Lambda"),", which can turn a function to an ",Object(o.b)("inlineCode",{parentName:"li"},"nn.Module"),"."),Object(o.b)("li",{parentName:"ul"},Object(o.b)("inlineCode",{parentName:"li"},"UpsampleConv2d"),", which can be used to upsample the input image."),Object(o.b)("li",{parentName:"ul"},Object(o.b)("inlineCode",{parentName:"li"},"interpolate"),", which is a handy function to resize the input image to the desired size.")))),Object(o.b)("p",null,"After we finished implementing our model, we need to implement the special loss used in VAE tasks:"),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),'@cflearn.register_loss_module("simple_vae")\n@cflearn.register_loss_module("simple_vae_foo")\nclass SimpleVAELoss(cflearn.LossModule):\n    def forward(\n        self,\n        forward_results: tensor_dict_type,\n        batch: tensor_dict_type,\n        state: Optional[TrainerState] = None,\n        **kwargs: Any,\n    ) -> losses_type:\n        # reconstruction loss\n        original = batch[cflearn.INPUT_KEY]\n        reconstruction = forward_results[cflearn.PREDICTIONS_KEY]\n        mse = F.mse_loss(reconstruction, original)\n        # kld loss\n        mu = forward_results["mu"]\n        log_var = forward_results["log_var"]\n        kld_losses = -0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim=1)\n        kld_loss = torch.mean(kld_losses, dim=0)\n        # gather\n        loss = mse + 0.001 * kld_loss\n        return {"mse": mse, "kld": kld_loss, cflearn.LOSS_KEY: loss}\n')),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"We used ",Object(o.b)("inlineCode",{parentName:"li"},"register_loss_module")," to register a general ",Object(o.b)("inlineCode",{parentName:"li"},"LossModule")," instance to a ",Object(o.b)("inlineCode",{parentName:"li"},"LossProtocol")," in ",Object(o.b)("inlineCode",{parentName:"li"},"carefree-learn"),"."),Object(o.b)("li",{parentName:"ul"},"We can call ",Object(o.b)("inlineCode",{parentName:"li"},"register_loss_module")," multiple times to assign multiple names to the same loss function."),Object(o.b)("li",{parentName:"ul"},"When the loss function shares the same name with the model, we don't need to specify the ",Object(o.b)("inlineCode",{parentName:"li"},"loss_name")," argument explicitly:")),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),'# Notice that we don\'t need to explicitly specify `loss_name`!\ncflearn.api.fit_cv(\n    data,\n    "simple_vae",\n    {"in_channels": 1, "img_size": 28},\n    fixed_epoch=1,                                  # for demo purpose, we only train our model for 1 epoch\n    cuda=0 if torch.cuda.is_available() else None,  # use CUDA if possible\n)\n')),Object(o.b)("p",null,"Of course, we can still specify ",Object(o.b)("inlineCode",{parentName:"p"},"loss_name")," explicitly:"),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),'cflearn.api.fit_cv(\n    data,\n    "simple_vae",\n    {"in_channels": 1, "img_size": 28},\n    loss_name="simple_vae_foo",                     # we used the second registered name here\n    fixed_epoch=1,                                  # for demo purpose, we only train our model for 1 epoch\n    cuda=0 if torch.cuda.is_available() else None,  # use CUDA if possible\n)\n')),Object(o.b)("h2",{id:"generative-adversarial-network"},"Generative Adversarial Network"),Object(o.b)("table",null,Object(o.b)("thead",{parentName:"table"},Object(o.b)("tr",{parentName:"thead"},Object(o.b)("th",Object(a.a)({parentName:"tr"},{align:"center"}),"Python source code"),Object(o.b)("th",Object(a.a)({parentName:"tr"},{align:"center"}),"Jupyter Notebook"),Object(o.b)("th",Object(a.a)({parentName:"tr"},{align:"center"}),"Task"))),Object(o.b)("tbody",{parentName:"table"},Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",Object(a.a)({parentName:"tr"},{align:"center"}),Object(o.b)("a",Object(a.a)({parentName:"td"},{href:"https://github.com/carefree0910/carefree-learn/blob/dev/examples/cv/mnist/run_gan.py"}),"run_gan.py")),Object(o.b)("td",Object(a.a)({parentName:"tr"},{align:"center"}),Object(o.b)("a",Object(a.a)({parentName:"td"},{href:"https://nbviewer.jupyter.org/github/carefree0910/carefree-learn/blob/dev/examples/cv/mnist/gan.ipynb"}),"gan.ipynb")),Object(o.b)("td",Object(a.a)({parentName:"tr"},{align:"center"}),"Computer Vision \ud83d\uddbc\ufe0f")))),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),"from torch import Tensor\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Callable\nfrom typing import Optional\nfrom torch.optim import Optimizer\nfrom cflearn.types import tensor_dict_type\nfrom cflearn.protocol import StepOutputs\nfrom cflearn.protocol import TrainerState\nfrom cflearn.protocol import MetricsOutputs\nfrom cflearn.protocol import DataLoaderProtocol\nfrom cflearn.constants import INPUT_KEY\nfrom cflearn.constants import PREDICTIONS_KEY\nfrom cflearn.misc.toolkit import to_device\nfrom cflearn.misc.toolkit import interpolate\nfrom cflearn.misc.toolkit import toggle_optimizer\nfrom cflearn.modules.blocks import Lambda\nfrom cflearn.modules.blocks import UpsampleConv2d\nfrom torch.cuda.amp.grad_scaler import GradScaler\n")),Object(o.b)("p",null,"For demo purpose, we are going to build a simple convolution-based GAN. But first, let's build the loss function of GAN:"),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),'class GANLoss(nn.Module):\n    def __init__(self):  # type: ignore\n        super().__init__()\n        self.loss = nn.BCEWithLogitsLoss()\n        self.register_buffer("real_label", torch.tensor(1.0))\n        self.register_buffer("fake_label", torch.tensor(0.0))\n\n    def expand_target(self, tensor: Tensor, use_real_label: bool) -> Tensor:\n        target = self.real_label if use_real_label else self.fake_label\n        return target.expand_as(tensor)  # type: ignore\n\n    def forward(self, predictions: Tensor, use_real_label: bool) -> Tensor:\n        target_tensor = self.expand_target(predictions, use_real_label)\n        loss = self.loss(predictions, target_tensor)\n        return loss\n')),Object(o.b)("p",null,"Although the concept of GAN is fairly easy, it's pretty complicated if we want to implement it with a 'pre-defined' framework. In order to provide full flexibility, ",Object(o.b)("inlineCode",{parentName:"p"},"carefree-learn")," exposed two methods for users:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("inlineCode",{parentName:"li"},"train_step"),", which is used to control ",Object(o.b)("strong",{parentName:"li"},"ALL")," training behaviours, including:",Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},"calculate losses"),Object(o.b)("li",{parentName:"ul"},"apply back propagation"),Object(o.b)("li",{parentName:"ul"},"perform ",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/amp.html"}),"automatic mixed precision"),", ",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html"}),"gradient norm clipping")," and so on"))),Object(o.b)("li",{parentName:"ul"},Object(o.b)("inlineCode",{parentName:"li"},"evaluate_step"),", which is used to define the final metric that we want to monitor.")),Object(o.b)("p",null,"Besides, we also need to define the ",Object(o.b)("inlineCode",{parentName:"p"},"forward")," method, as usual."),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),'@cflearn.register_custom_module("simple_gan")\nclass SimpleGAN(cflearn.CustomModule):\n    def __init__(self, in_channels: int, img_size: int, latent_dim: int):\n        super().__init__()\n        if not latent_dim % 16 == 0:\n            raise ValueError(f"`latent_dim` ({latent_dim}) should be divided by 16")\n        self.latent_dim = latent_dim\n        latent_channels = latent_dim // 16\n        self.generator = nn.Sequential(\n            Lambda(lambda t: t.view(-1, latent_channels, 4, 4), name="reshape"),\n            nn.Conv2d(latent_channels, 128, 1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(128),\n            UpsampleConv2d(128, 64, kernel_size=3, padding=1, factor=2),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            UpsampleConv2d(64, 32, kernel_size=3, padding=1, factor=2),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(32),\n            UpsampleConv2d(32, in_channels, kernel_size=3, padding=1, factor=2),\n            Lambda(lambda t: interpolate(t, size=img_size, mode="bilinear")),\n        )\n        self.discriminator = nn.Sequential(\n            nn.Conv2d(in_channels, 16, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(16),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1),\n        )\n        self.loss = GANLoss()\n\n    def train_step(\n        self,\n        batch_idx: int,\n        batch: tensor_dict_type,\n        optimizers: Dict[str, Optimizer],\n        use_amp: bool,\n        grad_scaler: GradScaler,\n        clip_norm_fn: Callable[[], None],\n        scheduler_step_fn: Callable[[], None],\n        trainer: Any,\n        forward_kwargs: Dict[str, Any],\n        loss_kwargs: Dict[str, Any],\n    ) -> StepOutputs:\n        net = batch[INPUT_KEY]\n        # we will explain where do these keys come from in the following markdown block\n        opt_g = optimizers["core.g_parameters"]\n        opt_d = optimizers["core.d_parameters"]\n        # generator step\n        toggle_optimizer(self, opt_g)\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            sampled = self.sample(len(net))\n            pred_fake = self.discriminator(sampled)\n            g_loss = self.loss(pred_fake, use_real_label=True)\n        grad_scaler.scale(g_loss).backward()\n        clip_norm_fn()\n        grad_scaler.step(opt_g)\n        grad_scaler.update()\n        opt_g.zero_grad()\n        # discriminator step\n        toggle_optimizer(self, opt_d)\n        with torch.cuda.amp.autocast(enabled=use_amp):\n            pred_real = self.discriminator(net)\n            loss_d_real = self.loss(pred_real, use_real_label=True)\n            pred_fake = self.discriminator(sampled.detach().clone())\n            loss_d_fake = self.loss(pred_fake, use_real_label=False)\n            d_loss = 0.5 * (loss_d_fake + loss_d_real)\n        grad_scaler.scale(d_loss).backward()\n        clip_norm_fn()\n        grad_scaler.step(opt_d)\n        grad_scaler.update()\n        opt_d.zero_grad()\n        # finalize\n        scheduler_step_fn()\n        forward_results = {PREDICTIONS_KEY: sampled}\n        loss_dict = {\n            "g": g_loss.item(),\n            "d": d_loss.item(),\n            "d_fake": loss_d_fake.item(),\n            "d_real": loss_d_real.item(),\n        }\n        return StepOutputs(forward_results, loss_dict)\n\n    def evaluate_step(\n        self,\n        loader: DataLoaderProtocol,\n        portion: float,\n        weighted_loss_score_fn: Callable[[Dict[str, float]], float],\n        trainer: Any,\n    ) -> MetricsOutputs:\n        loss_items: Dict[str, List[float]] = {}\n        for i, batch in enumerate(loader):\n            if i / len(loader) >= portion:\n                break\n            batch = to_device(batch, self.device)\n            net = batch[INPUT_KEY]\n            sampled = self.sample(len(net))\n            pred_fake = self.discriminator(sampled)\n            g_loss = self.loss(pred_fake, use_real_label=True)\n            pred_real = self.discriminator(net)\n            d_loss = self.loss(pred_real, use_real_label=True)\n            loss_items.setdefault("g", []).append(g_loss.item())\n            loss_items.setdefault("d", []).append(d_loss.item())\n        # gather\n        mean_loss_items = {k: sum(v) / len(v) for k, v in loss_items.items()}\n        mean_loss_items[cflearn.LOSS_KEY] = sum(mean_loss_items.values())\n        score = weighted_loss_score_fn(mean_loss_items)\n        return MetricsOutputs(score, mean_loss_items)\n\n    @property\n    def g_parameters(self) -> List[nn.Parameter]:\n        return list(self.generator.parameters())\n\n    @property\n    def d_parameters(self) -> List[nn.Parameter]:\n        return list(self.discriminator.parameters())\n\n    def sample(self, num_samples: int) -> Tensor:\n        z = torch.randn(num_samples, self.latent_dim, device=self.device)\n        return self.generator(z)\n\n    def forward(\n        self,\n        batch_idx: int,\n        batch: tensor_dict_type,\n        state: Optional[TrainerState] = None,\n        **kwargs: Any,\n    ) -> tensor_dict_type:\n        return {PREDICTIONS_KEY: self.sample(len(batch[INPUT_KEY]))}\n')),Object(o.b)("p",null,"We leveraged the ",Object(o.b)("inlineCode",{parentName:"p"},"register_custom_module")," API here, which can turn a general ",Object(o.b)("inlineCode",{parentName:"p"},"CustomModule")," instance to a ",Object(o.b)("a",Object(a.a)({parentName:"p"},{href:"/docs/design-principles#model"}),Object(o.b)("inlineCode",{parentName:"a"},"ModelProtocol"))," in ",Object(o.b)("inlineCode",{parentName:"p"},"carefree-learn"),". After registered, it can be easily accessed with its name (",Object(o.b)("inlineCode",{parentName:"p"},'"simple_gan"'),")."),Object(o.b)("p",null,"There are two more things that are worth mentioning:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},"When using models with custom steps, we don't need to specify ",Object(o.b)("inlineCode",{parentName:"li"},"loss_name")," anymore, because the losses are calculated inside ",Object(o.b)("inlineCode",{parentName:"li"},"train_step"),"."),Object(o.b)("li",{parentName:"ul"},"The ",Object(o.b)("inlineCode",{parentName:"li"},"register_custom_module")," API will generate a ",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"/docs/design-principles#model"}),Object(o.b)("inlineCode",{parentName:"a"},"ModelProtocol")),", whose ",Object(o.b)("inlineCode",{parentName:"li"},"core")," property points to the original ",Object(o.b)("inlineCode",{parentName:"li"},"CustomModule"),". From the above codes, we can see that ",Object(o.b)("inlineCode",{parentName:"li"},"SimpleGAN")," implements ",Object(o.b)("inlineCode",{parentName:"li"},"g_parameters")," and ",Object(o.b)("inlineCode",{parentName:"li"},"d_parameters"),", which means the ",Object(o.b)("inlineCode",{parentName:"li"},"self.core.g_parameters")," and ",Object(o.b)("inlineCode",{parentName:"li"},"self.core.d_parameters")," of the generated ",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"/docs/design-principles#model"}),Object(o.b)("inlineCode",{parentName:"a"},"ModelProtocol"))," will be two sets of parameters that we wish to optimize.",Object(o.b)("ul",{parentName:"li"},Object(o.b)("li",{parentName:"ul"},"In this case, the ",Object(o.b)("inlineCode",{parentName:"li"},"core.g_parameter")," and ",Object(o.b)("inlineCode",{parentName:"li"},"core.d_parameters")," will be the optimize ",Object(o.b)("inlineCode",{parentName:"li"},"scope")," of the generated ",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"/docs/design-principles#model"}),Object(o.b)("inlineCode",{parentName:"a"},"ModelProtocol")),". That's why we access the optimizers with them."),Object(o.b)("li",{parentName:"ul"},"Please refer to the ",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"/docs/getting-started/configurations#optimizerpack"}),Object(o.b)("inlineCode",{parentName:"a"},"OptimizerPack"))," section for more details.")))),Object(o.b)("pre",null,Object(o.b)("code",Object(a.a)({parentName:"pre"},{className:"language-python"}),'# Notice that we don\'t need to explicitly specify `loss_name`!\ncflearn.api.fit_cv(\n    data,\n    "simple_gan",\n    {"in_channels": 1, "img_size": 28, "latent_dim": 128},\n    optimizer_settings={\n        "core.g_parameters": {\n            "optimizer": "adam",\n            "scheduler": "warmup",\n        },\n        "core.d_parameters": {\n            "optimizer": "adam",\n            "scheduler": "warmup",\n        },\n    },\n    fixed_epoch=1,                                  # for demo purpose, we only train our model for 1 epoch\n    cuda=0 if torch.cuda.is_available() else None,  # use CUDA if possible\n)\n')))}p.isMDXComponent=!0},97:function(e,n,t){"use strict";t.d(n,"a",(function(){return d})),t.d(n,"b",(function(){return u}));var a=t(0),r=t.n(a);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var c=r.a.createContext({}),p=function(e){var n=r.a.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},d=function(e){var n=p(e.components);return r.a.createElement(c.Provider,{value:n},e.children)},m={inlineCode:"code",wrapper:function(e){var n=e.children;return r.a.createElement(r.a.Fragment,{},n)}},b=r.a.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,i=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=p(t),b=a,u=d["".concat(i,".").concat(b)]||d[b]||m[b]||o;return t?r.a.createElement(u,l(l({ref:n},c),{},{components:t})):r.a.createElement(u,l({ref:n},c))}));function u(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,i=new Array(o);i[0]=b;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,i[1]=l;for(var c=2;c<o;c++)i[c]=t[c];return r.a.createElement.apply(null,i)}return r.a.createElement.apply(null,t)}b.displayName="MDXCreateElement"}}]);